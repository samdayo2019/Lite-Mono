{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dummy input tensor of size: (1, 6, 224, 224)\n",
      "Ran ResidualAdd\n",
      "Warning: Error calculating FLOPs for <class 'torchvision.models.residual_add.ResidualAdd'>: too many values to unpack (expected 3)\n",
      "Ran ResidualAdd\n",
      "Warning: Error calculating FLOPs for <class 'torchvision.models.residual_add.ResidualAdd'>: too many values to unpack (expected 3)\n",
      "Ran ResidualAdd\n",
      "Warning: Error calculating FLOPs for <class 'torchvision.models.residual_add.ResidualAdd'>: too many values to unpack (expected 3)\n",
      "Ran ResidualAdd\n",
      "Warning: Error calculating FLOPs for <class 'torchvision.models.residual_add.ResidualAdd'>: too many values to unpack (expected 3)\n",
      "Ran ResidualAdd\n",
      "Warning: Error calculating FLOPs for <class 'torchvision.models.residual_add.ResidualAdd'>: too many values to unpack (expected 3)\n",
      "Ran ResidualAdd\n",
      "Warning: Error calculating FLOPs for <class 'torchvision.models.residual_add.ResidualAdd'>: too many values to unpack (expected 3)\n",
      "Ran ResidualAdd\n",
      "Warning: Error calculating FLOPs for <class 'torchvision.models.residual_add.ResidualAdd'>: too many values to unpack (expected 3)\n",
      "Ran ResidualAdd\n",
      "Warning: Error calculating FLOPs for <class 'torchvision.models.residual_add.ResidualAdd'>: too many values to unpack (expected 3)\n",
      "Extracted DAG for process_pose\n",
      "Num Nodes: 74, Num Edges: 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W111 14:04:51.055985295 NNPACK.cpp:61] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from resnet_encoder import ResnetEncoder\n",
    "from pose_decoder import PoseDecoder\n",
    "from depth_decoder import Upsampling, DepthDecoder, ExtractInitial, ExtractSecond, ExtractThird\n",
    "# from depth_encoder import LayerNorm, MatrixMultiply, Softmax, WeightedSum, LiteMono, Permute4d, GammaMultiply\n",
    "import depth_encoder\n",
    "# from residual_add import ResidualAdd\n",
    "from torchvision.models.residual_add import ResidualAdd\n",
    "from typing import Union\n",
    "import onnx\n",
    "import onnx.helper as helper\n",
    "from timm.models.layers import DropPath\n",
    "\n",
    "class NumpyFloatEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.float32, np.float64, np.int64)):\n",
    "            return float(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "class ProcessBatch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProcessBatch, self).__init__()\n",
    "        self.DepthEncoding = depth_encoder.LiteMono()\n",
    "        self.DepthDecoding = DepthDecoder(self.DepthEncoding.num_ch_enc, scales = range(3))\n",
    "        self._annotate_submodules(self.DepthEncoding, 'DepthEncoding')\n",
    "        self._annotate_submodules(self.DepthDecoding, 'DepthDecoding')\n",
    "        # self.PoseEncoding = ResnetEncoder(num_layers=18, pretrained=False, num_input_images=2)\n",
    "        # self.PoseDecoding = PoseDecoder(num_ch_enc=self.PoseEncoding.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
    "    def _annotate_submodules(self, module, model_name: str):\n",
    "        for name, sub_module in module.named_modules():\n",
    "            sub_module._name = model_name\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        features = []\n",
    "        x = (x - 0.45) / 0.225\n",
    "\n",
    "        x_down = []\n",
    "        for i in range(3):\n",
    "            x_down.append(self.DepthEncoding.input_downsample[i](x)) # generates 4 different levels of avg pooling\n",
    "\n",
    "        tmp_x = []\n",
    "        x = self.DepthEncoding.downsample_layers[0](x) # 3 sequential conv layers\n",
    "        x = self.DepthEncoding.cat1(x, x_down[0]) # concatenate 3 x 112 x 112 and 48 x 112 x 112 into 51 x 112 x 112\n",
    "        x = self.DepthEncoding.stem2(x) # contatenate (3 x 112 x 112 and 48 x 112 x 112 into 51 x 112 x 112) conv outputs 48 channels\n",
    "        # x = self.stem2(torch.cat((x, x_down[0]), dim=1)) # contatenate (3 x 112 x 112 and 48 x 112 x 112 into 51 x 112 x 112) conv outputs 48 channels\n",
    "        tmp_x.append(x) # append 51 x 112 x 112 to the front\n",
    "\n",
    "        for s in range(len(self.DepthEncoding.stages[0])-1):\n",
    "            x = self.DepthEncoding.stages[0][s](x) # iterating through the Dilated COnvs and LGFI blocks\n",
    "        x = self.DepthEncoding.stages[0][-1](x) # Stage 1 output --> \n",
    "        x2 = x\n",
    "        tmp_x.append(x)\n",
    "        features.append(x) # stage 1 output x2\n",
    "\n",
    "        # Unroll for loop for DConvs and LGFIs\n",
    "\n",
    "        tmp_x.append(x_down[1])\n",
    "        x = self.DepthEncoding.cat2(*tmp_x)\n",
    "        # x = torch.cat(tmp_x, dim=1)\n",
    "        x = self.DepthEncoding.downsample_layers[1](x)\n",
    "\n",
    "        tmp_x = [x]\n",
    "        for s in range(len(self.DepthEncoding.stages[1]) - 1):\n",
    "            x = self.DepthEncoding.stages[1][s](x)\n",
    "        x = self.DepthEncoding.stages[1][-1](x)\n",
    "        tmp_x.append(x)\n",
    "\n",
    "        features.append(x) # stage 2 output x1\n",
    "        x1 = x\n",
    "\n",
    "\n",
    "        tmp_x.append(x_down[2])\n",
    "        x = self.DepthEncoding.cat2(*tmp_x)\n",
    "        # x = torch.cat(tmp_x, dim=1)\n",
    "        x = self.DepthEncoding.downsample_layers[2](x)\n",
    "\n",
    "        tmp_x = [x]\n",
    "        for s in range(len(self.DepthEncoding.stages[2]) - 1):\n",
    "            x = self.DepthEncoding.stages[2][s](x)\n",
    "        x = self.DepthEncoding.stages[2][-1](x)\n",
    "        tmp_x.append(x)\n",
    "\n",
    "        features.append(x) # stage 3 output -> x\n",
    "\n",
    "\n",
    "        # Depth Decoder Code\n",
    "        self.outputs = {}\n",
    "        # input = features\n",
    "\n",
    "        # x = self.DepthDecoding.initialExtractor(input)\n",
    "        # x1 = self.DepthDecoding.secondExtractor(input)\n",
    "        # x2 = self.DepthDecoding.thirdExtractor(input)\n",
    "\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 2, 0)](x)\n",
    "        x = self.DepthDecoding.upsampler(x)\n",
    "        # x = self.listgen(x)\n",
    "        # x = [upsample(x)]\n",
    "\n",
    "        if self.DepthDecoding.use_skips:\n",
    "                x = self.DepthDecoding.cat_append(x, x1)\n",
    "            # else:\n",
    "            # y = self.initialExtractor(input_features, i - 1)\n",
    "            # # x += [input_features[i - 1]] # appending input_features to the upsampele list\n",
    "            # x = self.cat_append(x, y)\n",
    "        # x = torch.cat(x, 1)\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 2, 1)](x)\n",
    "\n",
    "        if 2 in self.DepthDecoding.scales:\n",
    "            f = self.DepthDecoding.convs[(\"dispconv\", 2)](x)\n",
    "            f = self.DepthDecoding.upsampler2(f)\n",
    "            # f = upsample(self.convs[(\"dispconv\", i)](x), mode='bilinear')\n",
    "            self.outputs[(\"disp\", 2)] = self.DepthDecoding.sigmoid(f)\n",
    "\n",
    "        #------- next loop\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 1, 0)](x)\n",
    "        x = self.DepthDecoding.upsampler(x)\n",
    "        # x = self.listgen(x)\n",
    "        # x = [upsample(x)]\n",
    "\n",
    "        if self.DepthDecoding.use_skips:\n",
    "                x = self.DepthDecoding.cat_append(x, x2)\n",
    "            # else:\n",
    "            # y = self.initialExtractor(input_features, i - 1)\n",
    "            # # x += [input_features[i - 1]] # appending input_features to the upsampele list\n",
    "            # x = self.cat_append(x, y)\n",
    "        # x = torch.cat(x, 1)\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 1, 1)](x)\n",
    "\n",
    "        if 1 in self.DepthDecoding.scales:\n",
    "            f = self.DepthDecoding.convs[(\"dispconv\", 1)](x)\n",
    "            f = self.DepthDecoding.upsampler2(f)\n",
    "            # f = upsample(self.convs[(\"dispconv\", i)](x), mode='bilinear')\n",
    "            self.outputs[(\"disp\", 1)] = self.DepthDecoding.sigmoid(f)\n",
    "\n",
    "        #------- next loop\n",
    "\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 0, 0)](x)\n",
    "        x = self.DepthDecoding.upsampler(x)\n",
    "        # x = self.listgen(x)\n",
    "        # x = [upsample(x)]\n",
    "\n",
    "           # else:\n",
    "            # y = self.initialExtractor(input_features, i - 1)\n",
    "            # # x += [input_features[i - 1]] # appending input_features to the upsampele list\n",
    "            # x = self.cat_append(x, y)\n",
    "        # x = torch.cat(x, 1)\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 0, 1)](x)\n",
    "\n",
    "        if 0 in self.DepthDecoding.scales:\n",
    "            f = self.DepthDecoding.convs[(\"dispconv\", 0)](x)\n",
    "            f = self.DepthDecoding.upsampler2(f)\n",
    "            # f = upsample(self.convs[(\"dispconv\", i)](x), mode='bilinear')\n",
    "            self.outputs[(\"disp\", 0)] = self.DepthDecoding.sigmoid(f)\n",
    "\n",
    "\n",
    "        # x = self.DepthEncoding(x)\n",
    "        print(f\"{len(features)}, {features[0].shape}, {features[1].shape}, {features[2].shape}\")\n",
    "        print(\"Outputs\")\n",
    "        print(f\"{len(self.outputs)}, {self.outputs[('disp', 0)].shape}, {self.outputs[('disp', 1)].shape}, {self.outputs[('disp', 2)].shape}\")\n",
    "        # x = self.DepthDecoding(x)\n",
    "        return self.outputs\n",
    "\n",
    "class ProcessPose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProcessPose, self).__init__()\n",
    "        self.PoseEncoding = ResnetEncoder(num_layers=18, pretrained=False, num_input_images=2)\n",
    "        self.PoseDecoding = PoseDecoder(num_ch_enc=self.PoseEncoding.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
    "        self._annotate_submodules(self.PoseEncoding, 'PoseEncoding')\n",
    "        self._annotate_submodules(self.PoseDecoding, 'PoseDecoding')\n",
    "    def _annotate_submodules(self, module, model_name: str):\n",
    "        for name, sub_module in module.named_modules():\n",
    "            sub_module._name = model_name\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.PoseEncoding(x)\n",
    "        x = [x]\n",
    "        x = self.PoseDecoding(x)\n",
    "        return x\n",
    "\n",
    "# Hardware assumptions (example values)\n",
    "HARDWARE_CONFIG = {\n",
    "    'compute_throughput': 10e12,  # 10 TFLOPS\n",
    "    'compute_efficiency': 5e-12,  # 5 pJ per FLOP\n",
    "    'memory_bandwidth': 900e9,    # 900 GB/s\n",
    "    'memory_energy': 20e-12,      # 20 pJ per byte\n",
    "    'interconnect_bandwidth': 400e9,  # 400 Gbps\n",
    "    'interconnect_latency': 100e-9,   # 100ns base latency\n",
    "    'interconnect_energy': 1e-12,     # 1 pJ per bit\n",
    "}\n",
    "\n",
    "def count_flops(module: nn.Module, in_shape: Tuple[int, ...], out_shape: Tuple[int, ...]) -> int:\n",
    "    \"\"\"Enhanced FLOP counter for various operations\"\"\"\n",
    "    try:\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Handle case where input might be reshaped\n",
    "            if len(in_shape) == 3:\n",
    "                batch_size = 1\n",
    "                in_channels, in_h, in_w = in_shape\n",
    "            else:\n",
    "                batch_size, in_channels, in_h, in_w = in_shape\n",
    "                \n",
    "            if len(out_shape) == 3:\n",
    "                out_channels, out_h, out_w = out_shape\n",
    "            else:\n",
    "                _, out_channels, out_h, out_w = out_shape\n",
    "                \n",
    "            kernel_h, kernel_w = module.kernel_size\n",
    "            flops = (2 * kernel_h * kernel_w * (in_channels // module.groups) - 1) * out_h * out_w * out_channels\n",
    "            \n",
    "        elif isinstance(module, nn.Linear):\n",
    "            flops = (2 * module.in_features - 1) * module.out_features\n",
    "            \n",
    "        elif isinstance(module, nn.BatchNorm2d):\n",
    "            if len(in_shape) == 3:\n",
    "                channels, height, width = in_shape\n",
    "            else:\n",
    "                _, channels, height, width = in_shape\n",
    "            flops = 2 * channels * height * width\n",
    "            \n",
    "        elif isinstance(module, (nn.ReLU, nn.ReLU6)):\n",
    "            flops = np.prod(in_shape)\n",
    "        \n",
    "        elif isinstance(module, nn.GELU):\n",
    "            flops = 12 * np.prod(in_shape)\n",
    "\n",
    "        elif isinstance(module, nn.ELU):   \n",
    "            flops = 4 * np.prod(in_shape)\n",
    "\n",
    "        elif isinstance(module, nn.MaxPool2d):\n",
    "            if len(out_shape) == 3:\n",
    "                channels, height, width = out_shape\n",
    "            else:\n",
    "                _, channels, height, width = out_shape\n",
    "            kernel_size = np.prod(module.kernel_size) if isinstance(module.kernel_size, tuple) else module.kernel_size**2\n",
    "            flops = (kernel_size - 1) * channels * height * width\n",
    "            \n",
    "        elif isinstance(module, nn.AvgPool2d):\n",
    "            if len(out_shape) == 3:\n",
    "                channels, height, width = out_shape\n",
    "            else:\n",
    "                _, channels, height, width = out_shape\n",
    "            kernel_size = np.prod(module.kernel_size) if isinstance(module.kernel_size, tuple) else module.kernel_size**2\n",
    "            flops = kernel_size * channels * height * width\n",
    "            \n",
    "        # elif isinstance(module, (PoseDecoder, DepthDecoder)):\n",
    "        #     # For decoders, sum up the FLOPs of their submodules\n",
    "        #     flops = sum(count_flops(m, in_shape, out_shape) for m in module.modules() \n",
    "        #                 if isinstance(m, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)))\n",
    "        elif isinstance(module, ResidualAdd):\n",
    "            channels, height, width = in_shape\n",
    "            flops = height * width * channels\n",
    "        elif isinstance(module, depth_encoder.LayerNorm):\n",
    "            N_elements = np.prod(in_shape)\n",
    "            flops = 8 * N_elements\n",
    "        elif isinstance(module, depth_encoder.MatrixMultiply):\n",
    "            q_shape = in_shape\n",
    "            B, heads, d_h, N = q_shape\n",
    "            flops = 2 * B * heads * N * d_h * d_h\n",
    "        elif isinstance(module, depth_encoder.Softmax):\n",
    "            B, heads, dh, _ = in_shape\n",
    "            flops = 2 * B * heads * dh * dh\n",
    "        elif isinstance(module, depth_encoder.WeightedSum):\n",
    "            attn_shape = in_shape\n",
    "            B, heads, dh, _ = attn_shape\n",
    "            _, _, _, N = out_shape\n",
    "            flops = 2 * N * d_h * B * heads * d_h\n",
    "        elif isinstance(module, depth_encoder.GammaMultiply):\n",
    "            N_elements = np.prod(in_shape)\n",
    "            flops = N_elements\n",
    "        elif isinstance(module, depth_encoder.PosEncode):\n",
    "            B, H_spatial, W_spatial = in_shape  # Assuming in_shape = [B, H_spatial, W_spatial]\n",
    "            hidden_dim = module.hidden_dim\n",
    "            dim = module.dim\n",
    "\n",
    "            # 1. Bitwise NOT\n",
    "            flops = B * H_spatial * W_spatial  # ~mask\n",
    "\n",
    "            # 2. Cumulative Sums\n",
    "            flops += 2 * B * H_spatial * W_spatial  # y_embed and x_embed cumsum\n",
    "\n",
    "            # 3. Normalization and Scaling\n",
    "            flops += 2 * B * H_spatial * W_spatial  # y_embed and x_embed normalization and scaling\n",
    "\n",
    "            # 4. Dimension Transformation\n",
    "            flops += 4 * hidden_dim  # dim_t operations\n",
    "\n",
    "            # 5. Positional Embeddings Division\n",
    "            flops += 2 * B * H_spatial * W_spatial * hidden_dim  # pos_x and pos_y division\n",
    "\n",
    "            # 6. Sin and Cos Transformations\n",
    "            flops += 2 * B * H_spatial * W_spatial * hidden_dim  # pos_x sin/cos and pos_y sin/cos\n",
    "\n",
    "            # 7. Token Projection (1x1 Conv)\n",
    "            # FLOPs = 2 * out_channels * H_out * W_out * in_channels * 1 * 1\n",
    "            # in_channels = hidden_dim * 2\n",
    "            # out_channels = dim\n",
    "            flops += 2 * dim * H_spatial * W_spatial * (hidden_dim * 2) * 1 * 1  # 4 * dim * hidden_dim * H * W\n",
    "        else:\n",
    "            flops = 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error calculating FLOPs for {type(module)}: {str(e)}\")\n",
    "        flops = 0\n",
    "\n",
    "    return int(flops)  # Convert to int to avoid numpy types\n",
    "def calculate_tensor_bytes(shape: Tuple[int, ...], dtype=torch.float32) -> int:\n",
    "    \"\"\"Calculate memory size in bytes for a tensor\"\"\"\n",
    "    element_size = {\n",
    "        torch.float32: 4,\n",
    "        torch.float16: 2,\n",
    "        torch.int8: 1,\n",
    "    }.get(dtype, 4)\n",
    "    return int(np.prod(shape) * element_size)  # Convert to int\n",
    "\n",
    "def estimate_compute_metrics(flops: int) -> Dict[str, float]:\n",
    "    \"\"\"Estimate runtime and energy for computation\"\"\"\n",
    "    runtime = float(flops / HARDWARE_CONFIG['compute_throughput'])\n",
    "    energy = float(flops * HARDWARE_CONFIG['compute_efficiency'])\n",
    "    return {\n",
    "        'runtime': runtime,\n",
    "        'energy': energy\n",
    "    }\n",
    "\n",
    "def estimate_communication_metrics(bytes_transferred: int) -> Dict[str, float]:\n",
    "    \"\"\"Estimate runtime and energy for data transfer\"\"\"\n",
    "    bits_transferred = bytes_transferred * 8\n",
    "    transfer_time = float(bits_transferred / HARDWARE_CONFIG['interconnect_bandwidth'])\n",
    "    total_latency = float(HARDWARE_CONFIG['interconnect_latency'] + transfer_time)\n",
    "    energy = float(bits_transferred * HARDWARE_CONFIG['interconnect_energy'])\n",
    "    return {\n",
    "        'runtime': total_latency,\n",
    "        'energy': energy\n",
    "    }\n",
    "\n",
    "def build_onnx_from_json(json_nodes, json_edges):\n",
    "    graph_nodes = []\n",
    "    graph_inputs = []\n",
    "    graph_outputs = []\n",
    "    initializers = []\n",
    "    node_map = {} \n",
    "\n",
    "    for node in json_nodes:\n",
    "        node_id = node['id']\n",
    "        op_type = node['op_type']\n",
    "        node_name = f\"node_{node_id}\"\n",
    "        node_map[node_id] = node_name\n",
    "        input_names = []\n",
    "        for edge in json_edges:\n",
    "            if edge['destination'] == node_id:\n",
    "                input_names.append(f\"node_{edge['source']}_output\")\n",
    "        output_name = f\"{node_name}_output\"\n",
    "\n",
    "        onnx_node = helper.make_node(\n",
    "            op_type=op_type,\n",
    "            inputs=input_names,\n",
    "            outputs=[output_name],\n",
    "            name=node_name\n",
    "        )\n",
    "        graph_nodes.append(onnx_node)\n",
    "\n",
    "        if 'param_shapes' in node and node['weight_shapes']:\n",
    "            for idx, shape in enumerate(node['weight_shapes']):\n",
    "                param_name = f\"{node_name}_param_{idx}\"\n",
    "                initializer = helper.make_tensor(\n",
    "                    name=param_name,\n",
    "                    data_type=onnx.TensorProto.FLOAT,\n",
    "                    dims=shape,\n",
    "                    vals=np.random.rand(*shape).astype(np.float32).flatten()\n",
    "                )\n",
    "                initializers.append(initializer)\n",
    "\n",
    "    for edge in json_edges:\n",
    "        if edge['source'] not in node_map:  \n",
    "            input_name = f\"node_{edge['source']}_output\"\n",
    "            graph_inputs.append(helper.make_tensor_value_info(\n",
    "                input_name,\n",
    "                onnx.TensorProto.FLOAT,\n",
    "                edge['tensor_shape']\n",
    "            ))\n",
    "        if edge['destination'] not in node_map:  \n",
    "            output_name = f\"node_{edge['destination']}_output\"\n",
    "            graph_outputs.append(helper.make_tensor_value_info(\n",
    "                output_name,\n",
    "                onnx.TensorProto.FLOAT,\n",
    "                edge['tensor_shape']\n",
    "            ))\n",
    "\n",
    "    graph = helper.make_graph(\n",
    "        nodes=graph_nodes,\n",
    "        name=\"ReconstructedGraph\",\n",
    "        inputs=graph_inputs,\n",
    "        outputs=graph_outputs,\n",
    "        initializer=initializers\n",
    "    )\n",
    "\n",
    "    model = helper.make_model(graph, producer_name=\"json_to_onnx\")\n",
    "    return model\n",
    "\n",
    "class EnhancedDAGExtractor:\n",
    "    def __init__(self, model_name: str = 'model'):\n",
    "        self.nodes = []\n",
    "        self.edges = []\n",
    "        self.node_count = 0\n",
    "        self.tensor_shapes = {}\n",
    "        self.model_name = model_name\n",
    "    \n",
    "    def get_node_id(self) -> int:\n",
    "        # self.node_count += 1\n",
    "        return self.node_count\n",
    "    \n",
    "    def add_node(self, name: str, op_type: str, weight_shape: Tuple[int, ...], \n",
    "                flops: int, input_shape: Tuple[int, ...], output_shape: Tuple[int, ...]) -> int:\n",
    "        node_id = self.get_node_id()\n",
    "        self.node_count+=1\n",
    "        weight_bytes = calculate_tensor_bytes(weight_shape) if weight_shape else 0\n",
    "        compute_metrics = estimate_compute_metrics(flops)\n",
    "        \n",
    "        self.nodes.append({\n",
    "            \"id\": node_id,\n",
    "            \"name\": name,\n",
    "            \"op_type\": op_type,\n",
    "            \"weight_shape\": list(weight_shape) if weight_shape else [],\n",
    "            \"weight_bytes\": weight_bytes,\n",
    "            \"flops\": flops,\n",
    "            \"input_shape\": list(input_shape),\n",
    "            \"output_shape\": list(output_shape),\n",
    "            \"estimated_runtime\": compute_metrics['runtime'],\n",
    "            \"estimated_energy\": compute_metrics['energy']\n",
    "        })\n",
    "        return node_id\n",
    "    \n",
    "    def add_edge(self, source_id: int, dest_id: int, tensor_shape: Tuple[int, ...]):\n",
    "        tensor_bytes = calculate_tensor_bytes(tensor_shape)\n",
    "        comm_metrics = estimate_communication_metrics(tensor_bytes)\n",
    "        \n",
    "        self.edges.append({\n",
    "            \"source\": source_id,\n",
    "            \"destination\": dest_id,\n",
    "            \"tensor_shape\": list(tensor_shape),\n",
    "            \"tensor_bytes\": tensor_bytes,\n",
    "            \"estimated_latency\": comm_metrics['runtime'],\n",
    "            \"estimated_energy\": comm_metrics['energy']\n",
    "        })\n",
    "\n",
    "    def _extract_first_tensor_shape(self, data):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return tuple(data.shape)\n",
    "        \n",
    "        if isinstance(data, (tuple, list)) and len(data) > 0:\n",
    "            return self._extract_first_tensor_shape(data[0])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _add_edges_for_nested_input(self, inp, dest_id):\n",
    "        if isinstance(inp, torch.Tensor):\n",
    "            if inp in self.tensor_shapes:\n",
    "                source_id, tensor_shape = self.tensor_shapes[inp]\n",
    "                self.add_edge(source_id, dest_id, tensor_shape)\n",
    "            else:\n",
    "                pass  # Ignore tensors not seen before\n",
    "        elif isinstance(inp, (tuple, list)):\n",
    "            for i in inp:\n",
    "                self._add_edges_for_nested_input(i, dest_id)\n",
    "\n",
    "    def hook_fn(self, module, input_tensor, output_tensor):\n",
    "        node_id = self.get_node_id()\n",
    "        op_type = module.__class__.__name__\n",
    "        \n",
    "        # input_shape = tuple(input_tensor[0].shape)\n",
    "        # output_shape = tuple(output_tensor.shape)\n",
    "        # weight_shape = tuple(module.weight.shape) if hasattr(module, 'weight') else None\n",
    "        \n",
    "        input_shape = self._extract_first_tensor_shape(input_tensor)\n",
    "        output_shape = self._extract_first_tensor_shape(output_tensor)\n",
    "        weight_shape = tuple(module.weight.shape) if hasattr(module, 'weight') else None\n",
    "\n",
    "        flops = count_flops(module, input_shape, output_shape)\n",
    "\n",
    "        model_ident = getattr(module, '_name', self.model_name)\n",
    "        \n",
    "        self.add_node(\n",
    "            name=f\"{model_ident}_{op_type}_{node_id}\",\n",
    "            op_type=op_type,\n",
    "            weight_shape=weight_shape,\n",
    "            flops=flops,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape\n",
    "        )\n",
    "        \n",
    "        self.tensor_shapes[output_tensor] = (node_id, output_shape)\n",
    "\n",
    "        for inp in input_tensor:\n",
    "            # if inp in self.tensor_shapes:\n",
    "            #     source_id, tensor_shape = self.tensor_shapes[inp]\n",
    "            #     self.add_edge(source_id, node_id, tensor_shape)\n",
    "            self._add_edges_for_nested_input(inp, node_id)\n",
    "\n",
    "    def is_shape_tuple(self, x):\n",
    "        \"\"\"\n",
    "        Returns True if x is a tuple/list of ints, e.g. (1, 3, 224, 224).\n",
    "        Returns False otherwise.\n",
    "        \"\"\"\n",
    "        if not isinstance(x, (tuple, list)):\n",
    "            return False\n",
    "        return all(isinstance(el, int) for el in x)\n",
    "\n",
    "\n",
    "    def extract_dag(self, model: nn.Module, input_size: Union[Tuple[int, ...], List[torch.Tensor]]):\n",
    "        hooks = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.ReLU, \n",
    "                                nn.MaxPool2d, nn.AvgPool2d, nn.ReLU6, ResidualAdd, nn.GELU, nn.AvgPool2d, \n",
    "                                depth_encoder.LayerNorm, depth_encoder.MatrixMultiply, depth_encoder.Softmax, depth_encoder.WeightedSum,\n",
    "                                depth_encoder.GammaMultiply, DropPath, depth_encoder.PosEncode,\n",
    "                                depth_encoder.Reshape3d, depth_encoder.Reshape4d, depth_encoder.Reshape5d,\n",
    "                                depth_encoder.Permute3d, depth_encoder.Permute4d, depth_encoder.Permute5d, \n",
    "                                depth_encoder.Transpose2d, depth_encoder.Normalize2d, depth_encoder.Cat,\n",
    "                                depth_encoder.Extract2dq, depth_encoder.Extract2dv, depth_encoder.Extract2dk,\n",
    "                                nn.ReflectionPad2d, nn.ZeroPad2d, nn.ELU, nn.Sigmoid, Upsampling, ExtractInitial,\n",
    "                                ExtractSecond, ExtractThird)):\n",
    "                # if(isinstance(module, ResidualAdd)):\n",
    "                #     print(\"This is in extract DAG for ResidualAdd\")\n",
    "                hooks.append(module.register_forward_hook(self.hook_fn))\n",
    "        \n",
    "        if self.is_shape_tuple(input_size):\n",
    "            print(f\"Creating dummy input tensor of size: {input_size}\")\n",
    "            dummy_input = torch.randn(input_size)\n",
    "        else:\n",
    "            print(\"Using provided input tensor(s)\")\n",
    "            dummy_input = input_size\n",
    "\n",
    "        # # Handle both single tensor and list of tensor inputs\n",
    "        # if isinstance(input_size, (tuple, list)) and isinstance(input_size[0], torch.Tensor):\n",
    "        #     dummy_input = input_size  # Use provided tensors directly\n",
    "        # else:\n",
    "        #     dummy_input = torch.randn(input_size)  # Create new tensor\n",
    "            \n",
    "        model(dummy_input)\n",
    "        # print(\"Ran model with dummy input to extract DAG\")\n",
    "\n",
    "        \n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # print(\"Removed hooks after extracting DAG\")\n",
    "\n",
    "        onnx_model = build_onnx_from_json(self.nodes, self.edges)\n",
    "\n",
    "        # print(\"built onnx model\")\n",
    "\n",
    "        onnx.save(onnx_model, \"reconstructed_model_depthencoder.onnx\")\n",
    "        \n",
    "        # print(f\"Num Nodes: {len(self.nodes)}, Num Edges: {len(self.edges)}\")\n",
    "        return {\n",
    "            \"nodes\": self.nodes,\n",
    "            \"edges\": self.edges,\n",
    "            \"hardware_config\": HARDWARE_CONFIG\n",
    "        }\n",
    "\n",
    "def analyze_model(model_name: str, model: nn.Module, input_size: Tuple[int, ...]):\n",
    "    extractor = EnhancedDAGExtractor(model_name=model_name)\n",
    "    dag = extractor.extract_dag(model, input_size)\n",
    "\n",
    "    print(f\"Extracted DAG for {model_name}\")\n",
    "    print(f\"Num Nodes: {len(dag['nodes'])}, Num Edges: {len(dag['edges'])}\")\n",
    "    \n",
    "    with open(f'{model_name}_dag_enhanced.json', 'w') as f:\n",
    "        json.dump(dag, f, indent=2, cls=NumpyFloatEncoder)\n",
    "\n",
    "\n",
    "# input_features = torch.randn(1, 6, 224, 224)\n",
    "\n",
    "# # # Analyze ResNet18\n",
    "# resnet18 = ResnetEncoder(num_layers=18, pretrained=False, num_input_images=2)\n",
    "\n",
    "# encoder_features = resnet18(input_features)\n",
    "\n",
    "# # # # resnet18 = models.resnet18(pretrained=False)\n",
    "# # analyze_model('resnet18', resnet18, (1, 6, 224, 224))\n",
    "\n",
    "# # # Analyze PoseDecoder\n",
    "# # # num_ch_enc = np.array([64, 64, 128, 256, 512])  # Example encoder channels\n",
    "# num_input_features = 1  # Add this parameter\n",
    "# pose_decoder = PoseDecoder(\n",
    "#     num_ch_enc=resnet18.num_ch_enc,\n",
    "#     num_input_features=num_input_features, \n",
    "#     num_frames_to_predict_for=2\n",
    "# )\n",
    "# # # Create dummy input features list\n",
    "# pose_input_features = [\n",
    "#     encoder_features\n",
    "# ]\n",
    "\n",
    "# # print(\"Pose input tensor shape: \", len(encoder_features))\n",
    "\n",
    "# analyze_model('pose_decoder', pose_decoder, pose_input_features)\n",
    "\n",
    "# Analyze DepthDecoder\n",
    "# depth_decoder = DepthDecoder(\n",
    "#     num_ch_enc=num_ch_enc,\n",
    "#     scales=range(4),\n",
    "#     num_output_channels=1,\n",
    "#     use_skips=True\n",
    "# )\n",
    "# # Create dummy input features list\n",
    "# depth_input_features = [\n",
    "#     torch.randn(1, 64, 56, 56),    # First encoder feature\n",
    "#     torch.randn(1, 64, 28, 28),    # Second encoder feature\n",
    "#     torch.randn(1, 128, 14, 14),   # Third encoder feature\n",
    "#     torch.randn(1, 256, 7, 7),     # Fourth encoder feature\n",
    "#     torch.randn(1, 512, 7, 7)      # Fifth encoder feature\n",
    "# ]\n",
    "# analyze_model('depth_decoder', depth_decoder, depth_input_features)\n",
    "\n",
    "input_features = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "process_batch = ProcessBatch()\n",
    "\n",
    "# analyze_model('process_batch', process_batch, (1, 3, 224, 224))\n",
    "\n",
    "process_pose = ProcessPose()\n",
    "\n",
    "analyze_model('process_pose', process_pose, (1, 6, 224, 224))\n",
    "\n",
    "# depthencoder = depth_encoder.LiteMono() \n",
    "\n",
    "# # analyze_model('depth_encoder', depthencoder, (1, 3, 224, 224))\n",
    "\n",
    "# output_encoder = depthencoder(input_features)\n",
    "\n",
    "# depth_decoder = DepthDecoder(depthencoder.num_ch_enc, scales = range(3))\n",
    "\n",
    "# analyze_model('depth_decoder', depth_decoder, output_encoder)\n",
    "\n",
    "# print(depth_decoder(output_encoder))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
