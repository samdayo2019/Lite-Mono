{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sadayo24/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/sadayo24/miniforge3/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/home/sadayo24/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/sadayo24/miniforge3/lib/python3.12/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "randn(): argument 'size' (position 1) must be tuple of ints, but found element of type ResNet at pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 239\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Create dummy input features list\u001b[39;00m\n\u001b[1;32m    232\u001b[0m pose_input_features \u001b[38;5;241m=\u001b[39m [resnet18\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;66;03m# torch.randn(1, 64, 56, 56),    # First encoder feature\u001b[39;00m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# torch.randn(1, 64, 28, 28),    # Second encoder feature\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;66;03m# torch.randn(1, 512, 7, 7)      # Fifth encoder feature\u001b[39;00m\n\u001b[1;32m    238\u001b[0m ]\n\u001b[0;32m--> 239\u001b[0m \u001b[43manalyze_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpose_decoder\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpose_input_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Analyze DepthDecoder\u001b[39;00m\n\u001b[1;32m    242\u001b[0m depth_decoder \u001b[38;5;241m=\u001b[39m DepthDecoder(\n\u001b[1;32m    243\u001b[0m     num_ch_enc\u001b[38;5;241m=\u001b[39mnum_ch_enc,\n\u001b[1;32m    244\u001b[0m     scales\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m4\u001b[39m),\n\u001b[1;32m    245\u001b[0m     num_output_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    246\u001b[0m     use_skips\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    247\u001b[0m )\n",
      "Cell \u001b[0;32mIn[1], line 215\u001b[0m, in \u001b[0;36manalyze_model\u001b[0;34m(model_name, model, input_size)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_model\u001b[39m(model_name: \u001b[38;5;28mstr\u001b[39m, model: nn\u001b[38;5;241m.\u001b[39mModule, input_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]):\n\u001b[1;32m    214\u001b[0m     extractor \u001b[38;5;241m=\u001b[39m EnhancedDAGExtractor()\n\u001b[0;32m--> 215\u001b[0m     dag \u001b[38;5;241m=\u001b[39m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_dag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_dag_enhanced.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    218\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(dag, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39mNumpyFloatEncoder)\n",
      "Cell \u001b[0;32mIn[1], line 200\u001b[0m, in \u001b[0;36mEnhancedDAGExtractor.extract_dag\u001b[0;34m(self, model, input_size)\u001b[0m\n\u001b[1;32m    198\u001b[0m     dummy_input \u001b[38;5;241m=\u001b[39m input_size  \u001b[38;5;66;03m# Use provided tensors directly\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     dummy_input \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Create new tensor\u001b[39;00m\n\u001b[1;32m    202\u001b[0m model(dummy_input)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "\u001b[0;31mTypeError\u001b[0m: randn(): argument 'size' (position 1) must be tuple of ints, but found element of type ResNet at pos 0"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from pose_decoder import PoseDecoder\n",
    "from depth_decoder import DepthDecoder\n",
    "from typing import Union\n",
    "\n",
    "class NumpyFloatEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.float32, np.float64, np.int64)):\n",
    "            return float(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "# Hardware assumptions (example values)\n",
    "HARDWARE_CONFIG = {\n",
    "    'compute_throughput': 10e12,  # 10 TFLOPS\n",
    "    'compute_efficiency': 5e-12,  # 5 pJ per FLOP\n",
    "    'memory_bandwidth': 900e9,    # 900 GB/s\n",
    "    'memory_energy': 20e-12,      # 20 pJ per byte\n",
    "    'interconnect_bandwidth': 400e9,  # 400 Gbps\n",
    "    'interconnect_latency': 100e-9,   # 100ns base latency\n",
    "    'interconnect_energy': 1e-12,     # 1 pJ per bit\n",
    "}\n",
    "\n",
    "def count_flops(module: nn.Module, in_shape: Tuple[int, ...], out_shape: Tuple[int, ...]) -> int:\n",
    "    \"\"\"Enhanced FLOP counter for various operations\"\"\"\n",
    "    try:\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Handle case where input might be reshaped\n",
    "            if len(in_shape) == 3:\n",
    "                batch_size = 1\n",
    "                in_channels, in_h, in_w = in_shape\n",
    "            else:\n",
    "                batch_size, in_channels, in_h, in_w = in_shape\n",
    "                \n",
    "            if len(out_shape) == 3:\n",
    "                out_channels, out_h, out_w = out_shape\n",
    "            else:\n",
    "                _, out_channels, out_h, out_w = out_shape\n",
    "                \n",
    "            kernel_h, kernel_w = module.kernel_size\n",
    "            flops = (2 * kernel_h * kernel_w * (in_channels // module.groups) - 1) * out_h * out_w * out_channels\n",
    "            \n",
    "        elif isinstance(module, nn.Linear):\n",
    "            flops = (2 * module.in_features - 1) * module.out_features\n",
    "            \n",
    "        elif isinstance(module, nn.BatchNorm2d):\n",
    "            if len(in_shape) == 3:\n",
    "                channels, height, width = in_shape\n",
    "            else:\n",
    "                _, channels, height, width = in_shape\n",
    "            flops = 2 * channels * height * width\n",
    "            \n",
    "        elif isinstance(module, (nn.ReLU, nn.ReLU6)):\n",
    "            flops = np.prod(in_shape)\n",
    "            \n",
    "        elif isinstance(module, nn.MaxPool2d):\n",
    "            if len(out_shape) == 3:\n",
    "                channels, height, width = out_shape\n",
    "            else:\n",
    "                _, channels, height, width = out_shape\n",
    "            kernel_size = np.prod(module.kernel_size) if isinstance(module.kernel_size, tuple) else module.kernel_size**2\n",
    "            flops = (kernel_size - 1) * channels * height * width\n",
    "            \n",
    "        elif isinstance(module, nn.AvgPool2d):\n",
    "            if len(out_shape) == 3:\n",
    "                channels, height, width = out_shape\n",
    "            else:\n",
    "                _, channels, height, width = out_shape\n",
    "            kernel_size = np.prod(module.kernel_size) if isinstance(module.kernel_size, tuple) else module.kernel_size**2\n",
    "            flops = kernel_size * channels * height * width\n",
    "            \n",
    "        elif isinstance(module, (PoseDecoder, DepthDecoder)):\n",
    "            # For decoders, sum up the FLOPs of their submodules\n",
    "            flops = sum(count_flops(m, in_shape, out_shape) for m in module.modules() \n",
    "                        if isinstance(m, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)))\n",
    "        else:\n",
    "            flops = 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error calculating FLOPs for {type(module)}: {str(e)}\")\n",
    "        flops = 0\n",
    "\n",
    "    return int(flops)  # Convert to int to avoid numpy types\n",
    "def calculate_tensor_bytes(shape: Tuple[int, ...], dtype=torch.float32) -> int:\n",
    "    \"\"\"Calculate memory size in bytes for a tensor\"\"\"\n",
    "    element_size = {\n",
    "        torch.float32: 4,\n",
    "        torch.float16: 2,\n",
    "        torch.int8: 1,\n",
    "    }.get(dtype, 4)\n",
    "    return int(np.prod(shape) * element_size)  # Convert to int\n",
    "\n",
    "def estimate_compute_metrics(flops: int) -> Dict[str, float]:\n",
    "    \"\"\"Estimate runtime and energy for computation\"\"\"\n",
    "    runtime = float(flops / HARDWARE_CONFIG['compute_throughput'])\n",
    "    energy = float(flops * HARDWARE_CONFIG['compute_efficiency'])\n",
    "    return {\n",
    "        'runtime': runtime,\n",
    "        'energy': energy\n",
    "    }\n",
    "\n",
    "def estimate_communication_metrics(bytes_transferred: int) -> Dict[str, float]:\n",
    "    \"\"\"Estimate runtime and energy for data transfer\"\"\"\n",
    "    bits_transferred = bytes_transferred * 8\n",
    "    transfer_time = float(bits_transferred / HARDWARE_CONFIG['interconnect_bandwidth'])\n",
    "    total_latency = float(HARDWARE_CONFIG['interconnect_latency'] + transfer_time)\n",
    "    energy = float(bits_transferred * HARDWARE_CONFIG['interconnect_energy'])\n",
    "    return {\n",
    "        'runtime': total_latency,\n",
    "        'energy': energy\n",
    "    }\n",
    "\n",
    "class EnhancedDAGExtractor:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "        self.edges = []\n",
    "        self.node_count = 0\n",
    "        self.tensor_shapes = {}\n",
    "    \n",
    "    def get_node_id(self) -> int:\n",
    "        self.node_count += 1\n",
    "        return self.node_count - 1\n",
    "    \n",
    "    def add_node(self, name: str, op_type: str, weight_shape: Tuple[int, ...], \n",
    "                flops: int, input_shape: Tuple[int, ...], output_shape: Tuple[int, ...]) -> int:\n",
    "        node_id = self.get_node_id()\n",
    "        \n",
    "        weight_bytes = calculate_tensor_bytes(weight_shape) if weight_shape else 0\n",
    "        compute_metrics = estimate_compute_metrics(flops)\n",
    "        \n",
    "        self.nodes.append({\n",
    "            \"id\": node_id,\n",
    "            \"name\": name,\n",
    "            \"op_type\": op_type,\n",
    "            \"weight_shape\": list(weight_shape) if weight_shape else [],\n",
    "            \"weight_bytes\": weight_bytes,\n",
    "            \"flops\": flops,\n",
    "            \"input_shape\": list(input_shape),\n",
    "            \"output_shape\": list(output_shape),\n",
    "            \"estimated_runtime\": compute_metrics['runtime'],\n",
    "            \"estimated_energy\": compute_metrics['energy']\n",
    "        })\n",
    "        return node_id\n",
    "    \n",
    "    def add_edge(self, source_id: int, dest_id: int, tensor_shape: Tuple[int, ...]):\n",
    "        tensor_bytes = calculate_tensor_bytes(tensor_shape)\n",
    "        comm_metrics = estimate_communication_metrics(tensor_bytes)\n",
    "        \n",
    "        self.edges.append({\n",
    "            \"source\": source_id,\n",
    "            \"destination\": dest_id,\n",
    "            \"tensor_shape\": list(tensor_shape),\n",
    "            \"tensor_bytes\": tensor_bytes,\n",
    "            \"estimated_latency\": comm_metrics['runtime'],\n",
    "            \"estimated_energy\": comm_metrics['energy']\n",
    "        })\n",
    "\n",
    "    def hook_fn(self, module, input_tensor, output_tensor):\n",
    "        node_id = self.get_node_id()\n",
    "        op_type = module.__class__.__name__\n",
    "        \n",
    "        input_shape = tuple(input_tensor[0].shape)\n",
    "        output_shape = tuple(output_tensor.shape)\n",
    "        weight_shape = tuple(module.weight.shape) if hasattr(module, 'weight') else None\n",
    "        \n",
    "        flops = count_flops(module, input_shape, output_shape)\n",
    "        \n",
    "        self.add_node(\n",
    "            name=f\"{op_type}_{node_id}\",\n",
    "            op_type=op_type,\n",
    "            weight_shape=weight_shape,\n",
    "            flops=flops,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape\n",
    "        )\n",
    "        \n",
    "        self.tensor_shapes[output_tensor] = (node_id, output_shape)\n",
    "\n",
    "        for inp in input_tensor:\n",
    "            if inp in self.tensor_shapes:\n",
    "                source_id, tensor_shape = self.tensor_shapes[inp]\n",
    "                self.add_edge(source_id, node_id, tensor_shape)\n",
    "\n",
    "    def extract_dag(self, model: nn.Module, input_size: Union[Tuple[int, ...], List[torch.Tensor]]):\n",
    "        hooks = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.ReLU, \n",
    "                                nn.MaxPool2d, nn.AvgPool2d, nn.ReLU6,\n",
    "                                PoseDecoder, DepthDecoder)):\n",
    "                hooks.append(module.register_forward_hook(self.hook_fn))\n",
    "        \n",
    "        # Handle both single tensor and list of tensor inputs\n",
    "        if isinstance(input_size, (tuple, list)) and isinstance(input_size[0], torch.Tensor):\n",
    "            dummy_input = input_size  # Use provided tensors directly\n",
    "        else:\n",
    "            dummy_input = torch.randn(input_size)  # Create new tensor\n",
    "            \n",
    "        model(dummy_input)\n",
    "        \n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "        \n",
    "        return {\n",
    "            \"nodes\": self.nodes,\n",
    "            \"edges\": self.edges,\n",
    "            \"hardware_config\": HARDWARE_CONFIG\n",
    "        }\n",
    "\n",
    "def analyze_model(model_name: str, model: nn.Module, input_size: Tuple[int, ...]):\n",
    "    extractor = EnhancedDAGExtractor()\n",
    "    dag = extractor.extract_dag(model, input_size)\n",
    "    \n",
    "    with open(f'{model_name}_dag_enhanced.json', 'w') as f:\n",
    "        json.dump(dag, f, indent=2, cls=NumpyFloatEncoder)\n",
    "\n",
    "# Analyze ResNet18\n",
    "resnet18 = models.resnet18(pretrained=False)\n",
    "analyze_model('resnet18', resnet18, (1, 3, 224, 224))\n",
    "\n",
    "# Analyze PoseDecoder\n",
    "num_ch_enc = np.array([64, 64, 128, 256, 512])  # Example encoder channels\n",
    "num_input_features = 512  # Add this parameter\n",
    "pose_decoder = PoseDecoder(\n",
    "    num_ch_enc=num_ch_enc,\n",
    "    num_input_features=num_input_features\n",
    ")\n",
    "# Create dummy input features list\n",
    "pose_input_features = [\n",
    "    torch.randn(1, 64, 56, 56),    # First encoder feature\n",
    "    torch.randn(1, 64, 28, 28),    # Second encoder feature\n",
    "    torch.randn(1, 128, 14, 14),   # Third encoder feature\n",
    "    torch.randn(1, 256, 7, 7),     # Fourth encoder feature\n",
    "    torch.randn(1, 512, 7, 7)      # Fifth encoder feature\n",
    "]\n",
    "analyze_model('pose_decoder', pose_decoder, pose_input_features)\n",
    "\n",
    "# Analyze DepthDecoder\n",
    "depth_decoder = DepthDecoder(\n",
    "    num_ch_enc=num_ch_enc,\n",
    "    scales=range(4),\n",
    "    num_output_channels=1,\n",
    "    use_skips=True\n",
    ")\n",
    "# Create dummy input features list\n",
    "depth_input_features = [\n",
    "    torch.randn(1, 64, 56, 56),    # First encoder feature\n",
    "    torch.randn(1, 64, 28, 28),    # Second encoder feature\n",
    "    torch.randn(1, 128, 14, 14),   # Third encoder feature\n",
    "    torch.randn(1, 256, 7, 7),     # Fourth encoder feature\n",
    "    torch.randn(1, 512, 7, 7)      # Fifth encoder feature\n",
    "]\n",
    "analyze_model('depth_decoder', depth_decoder, depth_input_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
