{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran ResidualAdd\n",
      "Ran combined pad and conv\n",
      "Ran combined pad and conv\n",
      "Ran combined pad and conv\n",
      "Ran combined pad and conv\n",
      "Ran combined pad and conv\n",
      "Ran combined pad and conv\n",
      "3, torch.Size([1, 48, 56, 56]), torch.Size([1, 80, 28, 28]), torch.Size([1, 128, 14, 14])\n",
      "Outputs\n",
      "3, torch.Size([1, 1, 224, 224]), torch.Size([1, 1, 112, 112]), torch.Size([1, 1, 56, 56])\n",
      "=======================================================================================================================================\n",
      "Layer (type:depth-idx)                                       Input Shape               Output Shape              Param #\n",
      "=======================================================================================================================================\n",
      "ProcessBatch                                                 [1, 3, 224, 224]          [1, 1, 224, 224]          --\n",
      "├─LiteMono: 1-1                                              --                        --                        --\n",
      "│    └─ModuleList: 2-1                                       --                        --                        --\n",
      "│    │    └─AvgPool: 3-1                                     [1, 3, 224, 224]          [1, 3, 112, 112]          --\n",
      "│    │    │    └─ModuleList: 4-1                             --                        --                        --\n",
      "│    │    │    │    └─AvgPool2d: 5-1                         [1, 3, 224, 224]          [1, 3, 112, 112]          --\n",
      "│    │    └─AvgPool: 3-2                                     [1, 3, 224, 224]          [1, 3, 56, 56]            --\n",
      "│    │    │    └─ModuleList: 4-2                             --                        --                        --\n",
      "│    │    │    │    └─AvgPool2d: 5-2                         [1, 3, 224, 224]          [1, 3, 112, 112]          --\n",
      "│    │    │    │    └─AvgPool2d: 5-3                         [1, 3, 112, 112]          [1, 3, 56, 56]            --\n",
      "│    │    └─AvgPool: 3-3                                     [1, 3, 224, 224]          [1, 3, 28, 28]            --\n",
      "│    │    │    └─ModuleList: 4-3                             --                        --                        --\n",
      "│    │    │    │    └─AvgPool2d: 5-4                         [1, 3, 224, 224]          [1, 3, 112, 112]          --\n",
      "│    │    │    │    └─AvgPool2d: 5-5                         [1, 3, 112, 112]          [1, 3, 56, 56]            --\n",
      "│    │    │    │    └─AvgPool2d: 5-6                         [1, 3, 56, 56]            [1, 3, 28, 28]            --\n",
      "│    └─ModuleList: 2-10                                      --                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-4                                  [1, 3, 224, 224]          [1, 48, 112, 112]         --\n",
      "│    │    │    └─Conv: 4-4                                   [1, 3, 224, 224]          [1, 48, 112, 112]         --\n",
      "│    │    │    │    └─Conv2d: 5-7                            [1, 3, 224, 224]          [1, 48, 112, 112]         1,296\n",
      "│    │    │    │    └─BNGELU: 5-8                            [1, 48, 112, 112]         [1, 48, 112, 112]         96\n",
      "│    │    │    └─Conv: 4-5                                   [1, 48, 112, 112]         [1, 48, 112, 112]         --\n",
      "│    │    │    │    └─Conv2d: 5-9                            [1, 48, 112, 112]         [1, 48, 112, 112]         20,736\n",
      "│    │    │    │    └─BNGELU: 5-10                           [1, 48, 112, 112]         [1, 48, 112, 112]         96\n",
      "│    │    │    └─Conv: 4-6                                   [1, 48, 112, 112]         [1, 48, 112, 112]         --\n",
      "│    │    │    │    └─Conv2d: 5-11                           [1, 48, 112, 112]         [1, 48, 112, 112]         20,736\n",
      "│    │    │    │    └─BNGELU: 5-12                           [1, 48, 112, 112]         [1, 48, 112, 112]         96\n",
      "│    └─Cat: 2-3                                              [1, 48, 112, 112]         [1, 51, 112, 112]         --\n",
      "│    └─Sequential: 2-4                                       [1, 51, 112, 112]         [1, 48, 56, 56]           --\n",
      "│    │    └─Conv: 3-5                                        [1, 51, 112, 112]         [1, 48, 56, 56]           --\n",
      "│    │    │    └─Conv2d: 4-7                                 [1, 51, 112, 112]         [1, 48, 56, 56]           22,032\n",
      "│    └─ModuleList: 2-11                                      --                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-6                                  --                        --                        --\n",
      "│    │    │    └─DilatedConv: 4-8                            [1, 48, 56, 56]           [1, 48, 56, 56]           144\n",
      "│    │    │    │    └─CDilated: 5-13                         [1, 48, 56, 56]           [1, 48, 56, 56]           432\n",
      "│    │    │    │    └─BatchNorm2d: 5-14                      [1, 48, 56, 56]           [1, 48, 56, 56]           96\n",
      "│    │    │    │    └─Permute4d: 5-15                        [1, 48, 56, 56]           [1, 56, 56, 48]           --\n",
      "│    │    │    │    └─Linear: 5-16                           [1, 56, 56, 48]           [1, 56, 56, 288]          14,112\n",
      "│    │    │    │    └─GELU: 5-17                             [1, 56, 56, 288]          [1, 56, 56, 288]          --\n",
      "│    │    │    │    └─Linear: 5-18                           [1, 56, 56, 288]          [1, 56, 56, 48]           13,872\n",
      "│    │    │    │    └─GammaMultiply: 5-19                    [48]                      [1, 56, 56, 48]           --\n",
      "│    │    │    │    └─Permute4d: 5-20                        [1, 56, 56, 48]           [1, 48, 56, 56]           --\n",
      "│    │    │    │    └─Identity: 5-21                         [1, 48, 56, 56]           [1, 48, 56, 56]           --\n",
      "│    │    │    │    └─ResidualAdd: 5-22                      [1, 48, 56, 56]           [1, 48, 56, 56]           --\n",
      "│    │    │    └─DilatedConv: 4-9                            [1, 48, 56, 56]           [1, 48, 56, 56]           144\n",
      "│    │    │    │    └─CDilated: 5-23                         [1, 48, 56, 56]           [1, 48, 56, 56]           432\n",
      "│    │    │    │    └─BatchNorm2d: 5-24                      [1, 48, 56, 56]           [1, 48, 56, 56]           96\n",
      "│    │    │    │    └─Permute4d: 5-25                        [1, 48, 56, 56]           [1, 56, 56, 48]           --\n",
      "│    │    │    │    └─Linear: 5-26                           [1, 56, 56, 48]           [1, 56, 56, 288]          14,112\n",
      "│    │    │    │    └─GELU: 5-27                             [1, 56, 56, 288]          [1, 56, 56, 288]          --\n",
      "│    │    │    │    └─Linear: 5-28                           [1, 56, 56, 288]          [1, 56, 56, 48]           13,872\n",
      "│    │    │    │    └─GammaMultiply: 5-29                    [48]                      [1, 56, 56, 48]           --\n",
      "│    │    │    │    └─Permute4d: 5-30                        [1, 56, 56, 48]           [1, 48, 56, 56]           --\n",
      "│    │    │    │    └─DropPath: 5-31                         [1, 48, 56, 56]           [1, 48, 56, 56]           --\n",
      "│    │    │    │    └─ResidualAdd: 5-32                      [1, 48, 56, 56]           [1, 48, 56, 56]           --\n",
      "│    │    │    └─DilatedConv: 4-10                           [1, 48, 56, 56]           [1, 48, 56, 56]           144\n",
      "│    │    │    │    └─CDilated: 5-33                         [1, 48, 56, 56]           [1, 48, 56, 56]           432\n",
      "│    │    │    │    └─BatchNorm2d: 5-34                      [1, 48, 56, 56]           [1, 48, 56, 56]           96\n",
      "│    │    │    │    └─Permute4d: 5-35                        [1, 48, 56, 56]           [1, 56, 56, 48]           --\n",
      "│    │    │    │    └─Linear: 5-36                           [1, 56, 56, 48]           [1, 56, 56, 288]          14,112\n",
      "│    │    │    │    └─GELU: 5-37                             [1, 56, 56, 288]          [1, 56, 56, 288]          --\n",
      "│    │    │    │    └─Linear: 5-38                           [1, 56, 56, 288]          [1, 56, 56, 48]           13,872\n",
      "│    │    │    │    └─GammaMultiply: 5-39                    [48]                      [1, 56, 56, 48]           --\n",
      "│    │    │    │    └─Permute4d: 5-40                        [1, 56, 56, 48]           [1, 48, 56, 56]           --\n",
      "│    │    │    │    └─DropPath: 5-41                         [1, 48, 56, 56]           [1, 48, 56, 56]           --\n",
      "│    │    │    │    └─ResidualAdd: 5-42                      [1, 48, 56, 56]           [1, 48, 56, 56]           --\n",
      "│    │    │    └─LGFI: 4-11                                  [1, 48, 56, 56]           [1, 48, 56, 56]           96\n",
      "│    │    │    │    └─Reshape3d: 5-43                        [1, 48, 56, 56]           [1, 48, 3136]             --\n",
      "│    │    │    │    └─Permute3d: 5-44                        [1, 48, 3136]             [1, 3136, 48]             --\n",
      "│    │    │    │    └─PositionalEncodingFourier: 5-45        [1, 56, 56]               [1, 48, 56, 56]           3,120\n",
      "│    │    │    │    └─Reshape3d: 5-46                        [1, 48, 56, 56]           [1, 48, 3136]             --\n",
      "│    │    │    │    └─Permute3d: 5-47                        [1, 48, 3136]             [1, 3136, 48]             --\n",
      "│    │    │    │    └─ResidualAdd: 5-48                      [1, 3136, 48]             [1, 3136, 48]             --\n",
      "│    │    │    │    └─LayerNorm: 5-49                        [1, 3136, 48]             [1, 3136, 48]             96\n",
      "│    │    │    │    └─XCA: 5-50                              [1, 3136, 48]             [1, 3136, 48]             9,416\n",
      "│    │    │    │    └─GammaMultiply: 5-51                    [1, 3136, 48]             [1, 3136, 48]             --\n",
      "│    │    │    │    └─ResidualAdd: 5-52                      [1, 3136, 48]             [1, 3136, 48]             --\n",
      "│    │    │    │    └─Reshape4d: 5-53                        [1, 3136, 48]             [1, 56, 56, 48]           --\n",
      "│    │    │    │    └─LayerNorm: 5-54                        [1, 56, 56, 48]           [1, 56, 56, 48]           96\n",
      "│    │    │    │    └─Linear: 5-55                           [1, 56, 56, 48]           [1, 56, 56, 288]          14,112\n",
      "│    │    │    │    └─GELU: 5-56                             [1, 56, 56, 288]          [1, 56, 56, 288]          --\n",
      "│    │    │    │    └─Linear: 5-57                           [1, 56, 56, 288]          [1, 56, 56, 48]           13,872\n",
      "│    │    │    │    └─GammaMultiply: 5-58                    [48]                      [1, 56, 56, 48]           --\n",
      "│    │    │    │    └─Permute4d: 5-59                        [1, 56, 56, 48]           [1, 48, 56, 56]           --\n",
      "│    │    │    │    └─DropPath: 5-60                         [1, 48, 56, 56]           [1, 48, 56, 56]           --\n",
      "│    │    │    │    └─ResidualAdd: 5-61                      [1, 48, 56, 56]           [1, 48, 56, 56]           --\n",
      "│    └─Cat: 2-6                                              [1, 48, 56, 56]           [1, 99, 56, 56]           --\n",
      "│    └─ModuleList: 2-10                                      --                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-7                                  [1, 99, 56, 56]           [1, 80, 28, 28]           --\n",
      "│    │    │    └─Conv: 4-12                                  [1, 99, 56, 56]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─Conv2d: 5-62                           [1, 99, 56, 56]           [1, 80, 28, 28]           71,280\n",
      "│    └─ModuleList: 2-11                                      --                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-8                                  --                        --                        --\n",
      "│    │    │    └─DilatedConv: 4-13                           [1, 80, 28, 28]           [1, 80, 28, 28]           240\n",
      "│    │    │    │    └─CDilated: 5-63                         [1, 80, 28, 28]           [1, 80, 28, 28]           720\n",
      "│    │    │    │    └─BatchNorm2d: 5-64                      [1, 80, 28, 28]           [1, 80, 28, 28]           160\n",
      "│    │    │    │    └─Permute4d: 5-65                        [1, 80, 28, 28]           [1, 28, 28, 80]           --\n",
      "│    │    │    │    └─Linear: 5-66                           [1, 28, 28, 80]           [1, 28, 28, 480]          38,880\n",
      "│    │    │    │    └─GELU: 5-67                             [1, 28, 28, 480]          [1, 28, 28, 480]          --\n",
      "│    │    │    │    └─Linear: 5-68                           [1, 28, 28, 480]          [1, 28, 28, 80]           38,480\n",
      "│    │    │    │    └─GammaMultiply: 5-69                    [80]                      [1, 28, 28, 80]           --\n",
      "│    │    │    │    └─Permute4d: 5-70                        [1, 28, 28, 80]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─DropPath: 5-71                         [1, 80, 28, 28]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─ResidualAdd: 5-72                      [1, 80, 28, 28]           [1, 80, 28, 28]           --\n",
      "│    │    │    └─DilatedConv: 4-14                           [1, 80, 28, 28]           [1, 80, 28, 28]           240\n",
      "│    │    │    │    └─CDilated: 5-73                         [1, 80, 28, 28]           [1, 80, 28, 28]           720\n",
      "│    │    │    │    └─BatchNorm2d: 5-74                      [1, 80, 28, 28]           [1, 80, 28, 28]           160\n",
      "│    │    │    │    └─Permute4d: 5-75                        [1, 80, 28, 28]           [1, 28, 28, 80]           --\n",
      "│    │    │    │    └─Linear: 5-76                           [1, 28, 28, 80]           [1, 28, 28, 480]          38,880\n",
      "│    │    │    │    └─GELU: 5-77                             [1, 28, 28, 480]          [1, 28, 28, 480]          --\n",
      "│    │    │    │    └─Linear: 5-78                           [1, 28, 28, 480]          [1, 28, 28, 80]           38,480\n",
      "│    │    │    │    └─GammaMultiply: 5-79                    [80]                      [1, 28, 28, 80]           --\n",
      "│    │    │    │    └─Permute4d: 5-80                        [1, 28, 28, 80]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─DropPath: 5-81                         [1, 80, 28, 28]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─ResidualAdd: 5-82                      [1, 80, 28, 28]           [1, 80, 28, 28]           --\n",
      "│    │    │    └─DilatedConv: 4-15                           [1, 80, 28, 28]           [1, 80, 28, 28]           240\n",
      "│    │    │    │    └─CDilated: 5-83                         [1, 80, 28, 28]           [1, 80, 28, 28]           720\n",
      "│    │    │    │    └─BatchNorm2d: 5-84                      [1, 80, 28, 28]           [1, 80, 28, 28]           160\n",
      "│    │    │    │    └─Permute4d: 5-85                        [1, 80, 28, 28]           [1, 28, 28, 80]           --\n",
      "│    │    │    │    └─Linear: 5-86                           [1, 28, 28, 80]           [1, 28, 28, 480]          38,880\n",
      "│    │    │    │    └─GELU: 5-87                             [1, 28, 28, 480]          [1, 28, 28, 480]          --\n",
      "│    │    │    │    └─Linear: 5-88                           [1, 28, 28, 480]          [1, 28, 28, 80]           38,480\n",
      "│    │    │    │    └─GammaMultiply: 5-89                    [80]                      [1, 28, 28, 80]           --\n",
      "│    │    │    │    └─Permute4d: 5-90                        [1, 28, 28, 80]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─DropPath: 5-91                         [1, 80, 28, 28]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─ResidualAdd: 5-92                      [1, 80, 28, 28]           [1, 80, 28, 28]           --\n",
      "│    │    │    └─LGFI: 4-16                                  [1, 80, 28, 28]           [1, 80, 28, 28]           160\n",
      "│    │    │    │    └─Reshape3d: 5-93                        [1, 80, 28, 28]           [1, 80, 784]              --\n",
      "│    │    │    │    └─Permute3d: 5-94                        [1, 80, 784]              [1, 784, 80]              --\n",
      "│    │    │    │    └─LayerNorm: 5-95                        [1, 784, 80]              [1, 784, 80]              160\n",
      "│    │    │    │    └─XCA: 5-96                              [1, 784, 80]              [1, 784, 80]              25,928\n",
      "│    │    │    │    └─GammaMultiply: 5-97                    [1, 784, 80]              [1, 784, 80]              --\n",
      "│    │    │    │    └─ResidualAdd: 5-98                      [1, 784, 80]              [1, 784, 80]              --\n",
      "│    │    │    │    └─Reshape4d: 5-99                        [1, 784, 80]              [1, 28, 28, 80]           --\n",
      "│    │    │    │    └─LayerNorm: 5-100                       [1, 28, 28, 80]           [1, 28, 28, 80]           160\n",
      "│    │    │    │    └─Linear: 5-101                          [1, 28, 28, 80]           [1, 28, 28, 480]          38,880\n",
      "│    │    │    │    └─GELU: 5-102                            [1, 28, 28, 480]          [1, 28, 28, 480]          --\n",
      "│    │    │    │    └─Linear: 5-103                          [1, 28, 28, 480]          [1, 28, 28, 80]           38,480\n",
      "│    │    │    │    └─GammaMultiply: 5-104                   [80]                      [1, 28, 28, 80]           --\n",
      "│    │    │    │    └─Permute4d: 5-105                       [1, 28, 28, 80]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─DropPath: 5-106                        [1, 80, 28, 28]           [1, 80, 28, 28]           --\n",
      "│    │    │    │    └─ResidualAdd: 5-107                     [1, 80, 28, 28]           [1, 80, 28, 28]           --\n",
      "│    └─Cat: 2-9                                              [1, 80, 28, 28]           [1, 163, 28, 28]          --\n",
      "│    └─ModuleList: 2-10                                      --                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-9                                  [1, 163, 28, 28]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─Conv: 4-17                                  [1, 163, 28, 28]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─Conv2d: 5-108                          [1, 163, 28, 28]          [1, 128, 14, 14]          187,776\n",
      "│    └─ModuleList: 2-11                                      --                        --                        (recursive)\n",
      "│    │    └─Sequential: 3-10                                 --                        --                        --\n",
      "│    │    │    └─DilatedConv: 4-18                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-109                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-110                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-111                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-112                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-113                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-114                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-115                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-116                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-117                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-118                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─DilatedConv: 4-19                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-119                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-120                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-121                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-122                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-123                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-124                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-125                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-126                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-127                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-128                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─DilatedConv: 4-20                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-129                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-130                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-131                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-132                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-133                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-134                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-135                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-136                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-137                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-138                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─DilatedConv: 4-21                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-139                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-140                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-141                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-142                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-143                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-144                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-145                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-146                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-147                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-148                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─DilatedConv: 4-22                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-149                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-150                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-151                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-152                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-153                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-154                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-155                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-156                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-157                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-158                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─DilatedConv: 4-23                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-159                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-160                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-161                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-162                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-163                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-164                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-165                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-166                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-167                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-168                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─DilatedConv: 4-24                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-169                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-170                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-171                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-172                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-173                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-174                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-175                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-176                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-177                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-178                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─DilatedConv: 4-25                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-179                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-180                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-181                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-182                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-183                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-184                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-185                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-186                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-187                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-188                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─DilatedConv: 4-26                           [1, 128, 14, 14]          [1, 128, 14, 14]          384\n",
      "│    │    │    │    └─CDilated: 5-189                        [1, 128, 14, 14]          [1, 128, 14, 14]          1,152\n",
      "│    │    │    │    └─BatchNorm2d: 5-190                     [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Permute4d: 5-191                       [1, 128, 14, 14]          [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Linear: 5-192                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-193                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-194                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-195                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-196                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-197                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-198                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    └─LGFI: 4-27                                  [1, 128, 14, 14]          [1, 128, 14, 14]          256\n",
      "│    │    │    │    └─Reshape3d: 5-199                       [1, 128, 14, 14]          [1, 128, 196]             --\n",
      "│    │    │    │    └─Permute3d: 5-200                       [1, 128, 196]             [1, 196, 128]             --\n",
      "│    │    │    │    └─LayerNorm: 5-201                       [1, 196, 128]             [1, 196, 128]             256\n",
      "│    │    │    │    └─XCA: 5-202                             [1, 196, 128]             [1, 196, 128]             66,056\n",
      "│    │    │    │    └─GammaMultiply: 5-203                   [1, 196, 128]             [1, 196, 128]             --\n",
      "│    │    │    │    └─ResidualAdd: 5-204                     [1, 196, 128]             [1, 196, 128]             --\n",
      "│    │    │    │    └─Reshape4d: 5-205                       [1, 196, 128]             [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─LayerNorm: 5-206                       [1, 14, 14, 128]          [1, 14, 14, 128]          256\n",
      "│    │    │    │    └─Linear: 5-207                          [1, 14, 14, 128]          [1, 14, 14, 768]          99,072\n",
      "│    │    │    │    └─GELU: 5-208                            [1, 14, 14, 768]          [1, 14, 14, 768]          --\n",
      "│    │    │    │    └─Linear: 5-209                          [1, 14, 14, 768]          [1, 14, 14, 128]          98,432\n",
      "│    │    │    │    └─GammaMultiply: 5-210                   [128]                     [1, 14, 14, 128]          --\n",
      "│    │    │    │    └─Permute4d: 5-211                       [1, 14, 14, 128]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─DropPath: 5-212                        [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "│    │    │    │    └─ResidualAdd: 5-213                     [1, 128, 14, 14]          [1, 128, 14, 14]          --\n",
      "├─DepthDecoder: 1-2                                          --                        --                        --\n",
      "│    └─ModuleList: 2-26                                      --                        --                        (recursive)\n",
      "│    │    └─ConvBlock: 3-11                                  [1, 128, 14, 14]          [1, 64, 14, 14]           --\n",
      "│    │    │    └─Sequential: 4-28                            [1, 128, 14, 14]          [1, 64, 14, 14]           --\n",
      "│    │    │    │    └─ReflectionPad2d: 5-214                 [1, 128, 14, 14]          [1, 128, 16, 16]          --\n",
      "│    │    │    │    └─Conv2d: 5-215                          [1, 128, 16, 16]          [1, 64, 14, 14]           73,792\n",
      "│    │    │    └─ELU: 4-29                                   [1, 64, 14, 14]           [1, 64, 14, 14]           --\n",
      "│    └─Upsampling: 2-13                                      [1, 64, 14, 14]           [1, 64, 28, 28]           --\n",
      "│    └─Cat: 2-14                                             [1, 64, 28, 28]           [1, 144, 28, 28]          --\n",
      "│    └─ModuleList: 2-26                                      --                        --                        (recursive)\n",
      "│    │    └─ConvBlock: 3-12                                  [1, 144, 28, 28]          [1, 64, 28, 28]           --\n",
      "│    │    │    └─Sequential: 4-30                            [1, 144, 28, 28]          [1, 64, 28, 28]           --\n",
      "│    │    │    │    └─ReflectionPad2d: 5-216                 [1, 144, 28, 28]          [1, 144, 30, 30]          --\n",
      "│    │    │    │    └─Conv2d: 5-217                          [1, 144, 30, 30]          [1, 64, 28, 28]           83,008\n",
      "│    │    │    └─ELU: 4-31                                   [1, 64, 28, 28]           [1, 64, 28, 28]           --\n",
      "│    │    └─Conv3x3: 3-13                                    [1, 64, 28, 28]           [1, 1, 28, 28]            --\n",
      "│    │    │    └─ReflectionPad2d: 4-32                       [1, 64, 28, 28]           [1, 64, 30, 30]           --\n",
      "│    │    │    └─Conv2d: 4-33                                [1, 64, 30, 30]           [1, 1, 28, 28]            577\n",
      "│    └─Upsampling: 2-16                                      [1, 1, 28, 28]            [1, 1, 56, 56]            --\n",
      "│    └─Sigmoid: 2-17                                         [1, 1, 56, 56]            [1, 1, 56, 56]            --\n",
      "│    └─ModuleList: 2-26                                      --                        --                        (recursive)\n",
      "│    │    └─ConvBlock: 3-14                                  [1, 64, 28, 28]           [1, 40, 28, 28]           --\n",
      "│    │    │    └─Sequential: 4-34                            [1, 64, 28, 28]           [1, 40, 28, 28]           --\n",
      "│    │    │    │    └─ReflectionPad2d: 5-218                 [1, 64, 28, 28]           [1, 64, 30, 30]           --\n",
      "│    │    │    │    └─Conv2d: 5-219                          [1, 64, 30, 30]           [1, 40, 28, 28]           23,080\n",
      "│    │    │    └─ELU: 4-35                                   [1, 40, 28, 28]           [1, 40, 28, 28]           --\n",
      "│    └─Upsampling: 2-19                                      [1, 40, 28, 28]           [1, 40, 56, 56]           --\n",
      "│    └─Cat: 2-20                                             [1, 40, 56, 56]           [1, 88, 56, 56]           --\n",
      "│    └─ModuleList: 2-26                                      --                        --                        (recursive)\n",
      "│    │    └─ConvBlock: 3-15                                  [1, 88, 56, 56]           [1, 40, 56, 56]           --\n",
      "│    │    │    └─Sequential: 4-36                            [1, 88, 56, 56]           [1, 40, 56, 56]           --\n",
      "│    │    │    │    └─ReflectionPad2d: 5-220                 [1, 88, 56, 56]           [1, 88, 58, 58]           --\n",
      "│    │    │    │    └─Conv2d: 5-221                          [1, 88, 58, 58]           [1, 40, 56, 56]           31,720\n",
      "│    │    │    └─ELU: 4-37                                   [1, 40, 56, 56]           [1, 40, 56, 56]           --\n",
      "│    │    └─Conv3x3: 3-16                                    [1, 40, 56, 56]           [1, 1, 56, 56]            --\n",
      "│    │    │    └─ReflectionPad2d: 4-38                       [1, 40, 56, 56]           [1, 40, 58, 58]           --\n",
      "│    │    │    └─Conv2d: 4-39                                [1, 40, 58, 58]           [1, 1, 56, 56]            361\n",
      "│    └─Upsampling: 2-22                                      [1, 1, 56, 56]            [1, 1, 112, 112]          --\n",
      "│    └─Sigmoid: 2-23                                         [1, 1, 112, 112]          [1, 1, 112, 112]          --\n",
      "│    └─ModuleList: 2-26                                      --                        --                        (recursive)\n",
      "│    │    └─ConvBlock: 3-17                                  [1, 40, 56, 56]           [1, 24, 56, 56]           --\n",
      "│    │    │    └─Sequential: 4-40                            [1, 40, 56, 56]           [1, 24, 56, 56]           --\n",
      "│    │    │    │    └─ReflectionPad2d: 5-222                 [1, 40, 56, 56]           [1, 40, 58, 58]           --\n",
      "│    │    │    │    └─Conv2d: 5-223                          [1, 40, 58, 58]           [1, 24, 56, 56]           8,664\n",
      "│    │    │    └─ELU: 4-41                                   [1, 24, 56, 56]           [1, 24, 56, 56]           --\n",
      "│    └─Upsampling: 2-25                                      [1, 24, 56, 56]           [1, 24, 112, 112]         --\n",
      "│    └─ModuleList: 2-26                                      --                        --                        (recursive)\n",
      "│    │    └─ConvBlock: 3-18                                  [1, 24, 112, 112]         [1, 24, 112, 112]         --\n",
      "│    │    │    └─Sequential: 4-42                            [1, 24, 112, 112]         [1, 24, 112, 112]         --\n",
      "│    │    │    │    └─ReflectionPad2d: 5-224                 [1, 24, 112, 112]         [1, 24, 114, 114]         --\n",
      "│    │    │    │    └─Conv2d: 5-225                          [1, 24, 114, 114]         [1, 24, 112, 112]         5,208\n",
      "│    │    │    └─ELU: 4-43                                   [1, 24, 112, 112]         [1, 24, 112, 112]         --\n",
      "│    │    └─Conv3x3: 3-19                                    [1, 24, 112, 112]         [1, 1, 112, 112]          --\n",
      "│    │    │    └─ReflectionPad2d: 4-44                       [1, 24, 112, 112]         [1, 24, 114, 114]         --\n",
      "│    │    │    └─Conv2d: 4-45                                [1, 24, 114, 114]         [1, 1, 112, 112]          217\n",
      "│    └─Upsampling: 2-27                                      [1, 1, 112, 112]          [1, 1, 224, 224]          --\n",
      "│    └─Sigmoid: 2-28                                         [1, 1, 224, 224]          [1, 1, 224, 224]          --\n",
      "=======================================================================================================================================\n",
      "Total params: 3,074,747\n",
      "Trainable params: 3,074,747\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.01\n",
      "=======================================================================================================================================\n",
      "Input size (MB): 0.60\n",
      "Forward/backward pass size (MB): 124.02\n",
      "Params size (MB): 12.28\n",
      "Estimated Total Size (MB): 136.90\n",
      "=======================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ptflops import get_model_complexity_info\n",
    "from torch.profiler import profile, ProfilerActivity, tensorboard_trace_handler\n",
    "import torchvision.models as models\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "from resnet_encoder import ResnetEncoder\n",
    "from pose_decoder import PoseDecoder\n",
    "from depth_decoder import Upsampling, DepthDecoder, ExtractInitial, ExtractSecond, ExtractThird\n",
    "# from depth_encoder import LayerNorm, MatrixMultiply, Softmax, WeightedSum, LiteMono, Permute4d, GammaMultiply\n",
    "import depth_encoder\n",
    "# from residual_add import ResidualAdd\n",
    "from torchvision.models.residual_add import ResidualAdd\n",
    "from typing import Union\n",
    "import onnx\n",
    "import onnx.helper as helper\n",
    "from timm.models.layers import DropPath\n",
    "from torchinfo import summary\n",
    "from torch.utils.flop_counter import FlopCounterMode\n",
    "\n",
    "systolicFlops = 0\n",
    "spatzFlops = 0\n",
    "\n",
    "weightsMem = 0\n",
    "actMem = 0\n",
    "\n",
    "\n",
    "class NumpyFloatEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.float32, np.float64, np.int64)):\n",
    "            return float(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "class ProcessBatch(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProcessBatch, self).__init__()\n",
    "        self.DepthEncoding = depth_encoder.LiteMono()\n",
    "        self.DepthDecoding = DepthDecoder(self.DepthEncoding.num_ch_enc, scales = range(3))\n",
    "        # self._annotate_submodules(self.DepthEncoding, 'DepthEncoding')\n",
    "        # self._annotate_submodules(self.DepthDecoding, 'DepthDecoding')\n",
    "        # self.PoseEncoding = ResnetEncoder(num_layers=18, pretrained=False, num_input_images=2)\n",
    "        # self.PoseDecoding = PoseDecoder(num_ch_enc=self.PoseEncoding.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
    "    # def _annotate_submodules(self, module, model_name: str):\n",
    "    #     for name, sub_module in module.named_modules():\n",
    "    #         sub_module._name = model_name\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        features = []\n",
    "        x = (x - 0.45) / 0.225\n",
    "\n",
    "        x_down = []\n",
    "        for i in range(3):\n",
    "            x_down.append(self.DepthEncoding.input_downsample[i](x)) # generates 4 different levels of avg pooling\n",
    "\n",
    "        tmp_x = []\n",
    "        x = self.DepthEncoding.downsample_layers[0](x) # 3 sequential conv layers\n",
    "        x = self.DepthEncoding.cat1(x, x_down[0]) # concatenate 3 x 112 x 112 and 48 x 112 x 112 into 51 x 112 x 112\n",
    "        x = self.DepthEncoding.stem2(x) # contatenate (3 x 112 x 112 and 48 x 112 x 112 into 51 x 112 x 112) conv outputs 48 channels\n",
    "        # x = self.stem2(torch.cat((x, x_down[0]), dim=1)) # contatenate (3 x 112 x 112 and 48 x 112 x 112 into 51 x 112 x 112) conv outputs 48 channels\n",
    "        tmp_x.append(x) # append 51 x 112 x 112 to the front\n",
    "\n",
    "        for s in range(len(self.DepthEncoding.stages[0])-1):\n",
    "            x = self.DepthEncoding.stages[0][s](x) # iterating through the Dilated COnvs and LGFI blocks\n",
    "        x = self.DepthEncoding.stages[0][-1](x) # Stage 1 output --> \n",
    "        x2 = x\n",
    "        tmp_x.append(x)\n",
    "        features.append(x) # stage 1 output x2\n",
    "\n",
    "        # Unroll for loop for DConvs and LGFIs\n",
    "\n",
    "        tmp_x.append(x_down[1])\n",
    "        x = self.DepthEncoding.cat2(*tmp_x)\n",
    "        # x = torch.cat(tmp_x, dim=1)\n",
    "        x = self.DepthEncoding.downsample_layers[1](x)\n",
    "\n",
    "        tmp_x = [x]\n",
    "        for s in range(len(self.DepthEncoding.stages[1]) - 1):\n",
    "            x = self.DepthEncoding.stages[1][s](x)\n",
    "        x = self.DepthEncoding.stages[1][-1](x)\n",
    "        tmp_x.append(x)\n",
    "\n",
    "        features.append(x) # stage 2 output x1\n",
    "        x1 = x\n",
    "\n",
    "\n",
    "        tmp_x.append(x_down[2])\n",
    "        x = self.DepthEncoding.cat2(*tmp_x)\n",
    "        # x = torch.cat(tmp_x, dim=1)\n",
    "        x = self.DepthEncoding.downsample_layers[2](x)\n",
    "\n",
    "        tmp_x = [x]\n",
    "        for s in range(len(self.DepthEncoding.stages[2]) - 1):\n",
    "            x = self.DepthEncoding.stages[2][s](x)\n",
    "        x = self.DepthEncoding.stages[2][-1](x)\n",
    "        tmp_x.append(x)\n",
    "\n",
    "        features.append(x) # stage 3 output -> x\n",
    "\n",
    "\n",
    "        # Depth Decoder Code\n",
    "        self.outputs = {}\n",
    "        # input = features\n",
    "\n",
    "        # x = self.DepthDecoding.initialExtractor(input)\n",
    "        # x1 = self.DepthDecoding.secondExtractor(input)\n",
    "        # x2 = self.DepthDecoding.thirdExtractor(input)\n",
    "\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 2, 0)](x)\n",
    "        x = self.DepthDecoding.upsampler(x)\n",
    "        # x = self.listgen(x)\n",
    "        # x = [upsample(x)]\n",
    "\n",
    "        if self.DepthDecoding.use_skips:\n",
    "                x = self.DepthDecoding.cat_append(x, x1)\n",
    "            # else:\n",
    "            # y = self.initialExtractor(input_features, i - 1)\n",
    "            # # x += [input_features[i - 1]] # appending input_features to the upsampele list\n",
    "            # x = self.cat_append(x, y)\n",
    "        # x = torch.cat(x, 1)\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 2, 1)](x)\n",
    "\n",
    "        if 2 in self.DepthDecoding.scales:\n",
    "            f = self.DepthDecoding.convs[(\"dispconv\", 2)](x)\n",
    "            f = self.DepthDecoding.upsampler2(f)\n",
    "            # f = upsample(self.convs[(\"dispconv\", i)](x), mode='bilinear')\n",
    "            self.outputs[(\"disp\", 2)] = self.DepthDecoding.sigmoid(f)\n",
    "\n",
    "        #------- next loop\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 1, 0)](x)\n",
    "        x = self.DepthDecoding.upsampler(x)\n",
    "        # x = self.listgen(x)\n",
    "        # x = [upsample(x)]\n",
    "\n",
    "        if self.DepthDecoding.use_skips:\n",
    "                x = self.DepthDecoding.cat_append(x, x2)\n",
    "            # else:\n",
    "            # y = self.initialExtractor(input_features, i - 1)\n",
    "            # # x += [input_features[i - 1]] # appending input_features to the upsampele list\n",
    "            # x = self.cat_append(x, y)\n",
    "        # x = torch.cat(x, 1)\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 1, 1)](x)\n",
    "\n",
    "        if 1 in self.DepthDecoding.scales:\n",
    "            f = self.DepthDecoding.convs[(\"dispconv\", 1)](x)\n",
    "            f = self.DepthDecoding.upsampler2(f)\n",
    "            # f = upsample(self.convs[(\"dispconv\", i)](x), mode='bilinear')\n",
    "            self.outputs[(\"disp\", 1)] = self.DepthDecoding.sigmoid(f)\n",
    "\n",
    "        #------- next loop\n",
    "\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 0, 0)](x)\n",
    "        x = self.DepthDecoding.upsampler(x)\n",
    "        # x = self.listgen(x)\n",
    "        # x = [upsample(x)]\n",
    "\n",
    "           # else:\n",
    "            # y = self.initialExtractor(input_features, i - 1)\n",
    "            # # x += [input_features[i - 1]] # appending input_features to the upsampele list\n",
    "            # x = self.cat_append(x, y)\n",
    "        # x = torch.cat(x, 1)\n",
    "        x = self.DepthDecoding.convs[(\"upconv\", 0, 1)](x)\n",
    "\n",
    "        if 0 in self.DepthDecoding.scales:\n",
    "            f = self.DepthDecoding.convs[(\"dispconv\", 0)](x)\n",
    "            f = self.DepthDecoding.upsampler2(f)\n",
    "            # f = upsample(self.convs[(\"dispconv\", i)](x), mode='bilinear')\n",
    "            self.outputs[(\"disp\", 0)] = self.DepthDecoding.sigmoid(f)\n",
    "\n",
    "\n",
    "        # x = self.DepthEncoding(x)\n",
    "        print(f\"{len(features)}, {features[0].shape}, {features[1].shape}, {features[2].shape}\")\n",
    "        print(\"Outputs\")\n",
    "        print(f\"{len(self.outputs)}, {self.outputs[('disp', 0)].shape}, {self.outputs[('disp', 1)].shape}, {self.outputs[('disp', 2)].shape}\")\n",
    "        # x = self.DepthDecoding(x)\n",
    "        return self.outputs\n",
    "\n",
    "class ProcessPose(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ProcessPose, self).__init__()\n",
    "        self.PoseEncoding = ResnetEncoder(num_layers=18, pretrained=False, num_input_images=2)\n",
    "        self.PoseDecoding = PoseDecoder(num_ch_enc=self.PoseEncoding.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
    "        self._annotate_submodules(self.PoseEncoding, 'PoseEncoding')\n",
    "        self._annotate_submodules(self.PoseDecoding, 'PoseDecoding')\n",
    "    def _annotate_submodules(self, module, model_name: str):\n",
    "        for name, sub_module in module.named_modules():\n",
    "            sub_module._name = model_name\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.PoseEncoding(x)\n",
    "        x = [x]\n",
    "        x = self.PoseDecoding(x)\n",
    "        return x\n",
    "\n",
    "# Hardware assumptions (example values)\n",
    "# HARDWARE_CONFIG = {\n",
    "#     'compute_throughput': 10e12,  # 10 TFLOPS\n",
    "#     'compute_efficiency': 5e-12,  # 5 pJ per FLOP\n",
    "#     'memory_bandwidth': 900e9,    # 900 GB/s\n",
    "#     'memory_energy': 25e-12,      # 20 pJ per byte\n",
    "#     'interconnect_bandwidth': 400e9,  # 400 Gbps\n",
    "#     'interconnect_latency': 100e-9,   # 100ns base latency\n",
    "#     'interconnect_energy': 1e-12,     # 1 pJ per bit\n",
    "# }\n",
    "HARDWARE_CONFIG = {\n",
    "    'compute_throughput': 1.6e12,  # 1.6 TFLOPS |-> Assuming 64 x 64 systolic array at 200 MHz clock\n",
    "    'compute_efficiency': 5e-12,  # 5 pJ per FLOP |-> Need to select a data type and put the energy per op in here \n",
    "    'memory_bandwidth': 900e9,    # 900 GB/s\n",
    "    'memory_energy': 25e-12,      # 20 pJ per byte\n",
    "    'interconnect_bandwidth': 6.4e9,  # 6.4 Gbps |-> 32 bit wide links at 200 MHz clock\n",
    "    'interconnect_latency': 100e-9,   # 100ns base latency\n",
    "    'interconnect_energy': 1e-12,     # 1 pJ per bit\n",
    "}\n",
    "\n",
    "def count_flops(module: nn.Module, in_shape: Tuple[int, ...], out_shape: Tuple[int, ...]) -> int:\n",
    "    \"\"\"Enhanced FLOP counter for various operations\"\"\"\n",
    "    global systolicFlops\n",
    "    global spatzFlops\n",
    "    try:\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            # Handle case where input might be reshaped\n",
    "            if len(in_shape) == 3:\n",
    "                batch_size = 1\n",
    "                in_channels, in_h, in_w = in_shape\n",
    "            else:\n",
    "                batch_size, in_channels, in_h, in_w = in_shape\n",
    "                \n",
    "            if len(out_shape) == 3:\n",
    "                out_channels, out_h, out_w = out_shape\n",
    "            else:\n",
    "                _, out_channels, out_h, out_w = out_shape\n",
    "                \n",
    "            kernel_h, kernel_w = module.kernel_size\n",
    "            flops = (2 * kernel_h * kernel_w * (in_channels // module.groups) - 1) * out_h * out_w * out_channels\n",
    "\n",
    "            systolicFlops += flops\n",
    "            \n",
    "        elif isinstance(module, nn.Linear):\n",
    "            flops = (2 * module.in_features - 1) * module.out_features\n",
    "\n",
    "            spatzFlops += flops\n",
    "            \n",
    "        elif isinstance(module, nn.BatchNorm2d):\n",
    "            if len(in_shape) == 3:\n",
    "                channels, height, width = in_shape\n",
    "            else:\n",
    "                _, channels, height, width = in_shape\n",
    "            flops = 2 * channels * height * width\n",
    "\n",
    "            spatzFlops += flops\n",
    "            \n",
    "        elif isinstance(module, (nn.ReLU, nn.ReLU6)):\n",
    "            flops = np.prod(in_shape)\n",
    "            spatzFlops += flops\n",
    "        \n",
    "        elif isinstance(module, nn.GELU):\n",
    "            flops = 12 * np.prod(in_shape)\n",
    "            spatzFlops += flops\n",
    "\n",
    "        elif isinstance(module, nn.ELU):   \n",
    "            flops = 4 * np.prod(in_shape)\n",
    "            spatzFlops += flops\n",
    "\n",
    "        elif isinstance(module, nn.MaxPool2d):\n",
    "            if len(out_shape) == 3:\n",
    "                channels, height, width = out_shape\n",
    "            else:\n",
    "                _, channels, height, width = out_shape\n",
    "            kernel_size = np.prod(module.kernel_size) if isinstance(module.kernel_size, tuple) else module.kernel_size**2\n",
    "            flops = (kernel_size - 1) * channels * height * width\n",
    "            spatzFlops += flops\n",
    "            \n",
    "        elif isinstance(module, nn.AvgPool2d):\n",
    "            if len(out_shape) == 3:\n",
    "                channels, height, width = out_shape\n",
    "            else:\n",
    "                _, channels, height, width = out_shape\n",
    "            kernel_size = np.prod(module.kernel_size) if isinstance(module.kernel_size, tuple) else module.kernel_size**2\n",
    "            flops = kernel_size * channels * height * width\n",
    "            spatzFlops += flops\n",
    "            \n",
    "        # elif isinstance(module, (PoseDecoder, DepthDecoder)):\n",
    "        #     # For decoders, sum up the FLOPs of their submodules\n",
    "        #     flops = sum(count_flops(m, in_shape, out_shape) for m in module.modules() \n",
    "        #                 if isinstance(m, (nn.Conv2d, nn.Linear, nn.BatchNorm2d)))\n",
    "        elif isinstance(module, ResidualAdd):\n",
    "            if len(in_shape) == 3:\n",
    "                channels, height, width = in_shape\n",
    "            else: \n",
    "                _, channels, height, width = in_shape\n",
    "            flops = height * width * channels\n",
    "            spatzFlops += flops\n",
    "        elif isinstance(module, depth_encoder.LayerNorm):\n",
    "            N_elements = np.prod(in_shape)\n",
    "            flops = 8 * N_elements\n",
    "            spatzFlops += flops\n",
    "        elif isinstance(module, depth_encoder.MatrixMultiply):\n",
    "            # q_shape = in_shape\n",
    "            _, _, _, d_h = in_shape\n",
    "            B, heads, N, _ = out_shape\n",
    "            flops = 2 * B * heads * N * d_h * d_h\n",
    "            systolicFlops += flops\n",
    "            spatzFlops += np.prod(out_shape)\n",
    "\n",
    "        elif isinstance(module, depth_encoder.Softmax):\n",
    "            B, heads, dh, _ = in_shape\n",
    "            flops = 2 * B * heads * dh * dh\n",
    "            spatzFlops += flops\n",
    "        elif isinstance(module, depth_encoder.WeightedSum):\n",
    "            # attn_shape = in_shape\n",
    "            # B, heads, d_h, _ = attn_shape\n",
    "            B, heads, N, d_h = out_shape\n",
    "            flops = 2 * N * d_h * B * heads * d_h\n",
    "            systolicFlops += flops  \n",
    "        elif isinstance(module, depth_encoder.GammaMultiply):\n",
    "            N_elements = np.prod(in_shape)\n",
    "            flops = N_elements\n",
    "            spatzFlops += flops\n",
    "        elif isinstance(module, depth_encoder.PosEncode):\n",
    "            B, H_spatial, W_spatial = in_shape  # Assuming in_shape = [B, H_spatial, W_spatial]\n",
    "            hidden_dim = module.hidden_dim\n",
    "            dim = module.dim\n",
    "\n",
    "            # 1. Bitwise NOT\n",
    "            flops = B * H_spatial * W_spatial  # ~mask\n",
    "\n",
    "            # 2. Cumulative Sums\n",
    "            flops += 2 * B * H_spatial * W_spatial  # y_embed and x_embed cumsum\n",
    "\n",
    "            # 3. Normalization and Scaling\n",
    "            flops += 2 * B * H_spatial * W_spatial  # y_embed and x_embed normalization and scaling\n",
    "\n",
    "            # 4. Dimension Transformation\n",
    "            flops += 4 * hidden_dim  # dim_t operations\n",
    "\n",
    "            # 5. Positional Embeddings Division\n",
    "            flops += 2 * B * H_spatial * W_spatial * hidden_dim  # pos_x and pos_y division\n",
    "\n",
    "            # 6. Sin and Cos Transformations\n",
    "            flops += 2 * B * H_spatial * W_spatial * hidden_dim  # pos_x sin/cos and pos_y sin/cos\n",
    "\n",
    "            # 7. Token Projection (1x1 Conv)\n",
    "            # FLOPs = 2 * out_channels * H_out * W_out * in_channels * 1 * 1\n",
    "            # in_channels = hidden_dim * 2\n",
    "            # out_channels = dim\n",
    "            flops += 2 * dim * H_spatial * W_spatial * (hidden_dim * 2) * 1 * 1  # 4 * dim * hidden_dim * H * W\n",
    "            spatzFlops += flops\n",
    "        else:\n",
    "            flops = 0\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error calculating FLOPs for {type(module)}: {str(e)}\")\n",
    "        flops = 0\n",
    "\n",
    "    return int(flops)  # Convert to int to avoid numpy types\n",
    "def calculate_tensor_bytes(shape: Tuple[int, ...], dtype=torch.float32) -> int:\n",
    "    \"\"\"Calculate memory size in bytes for a tensor\"\"\"\n",
    "    element_size = {\n",
    "        torch.float32: 4,\n",
    "        torch.float16: 2,\n",
    "        torch.int8: 1,\n",
    "    }.get(dtype, 4)\n",
    "    return int(np.prod(shape) * element_size)  # Convert to int\n",
    "\n",
    "def estimate_compute_metrics(flops: int, act_size: int) -> Dict[str, float]:\n",
    "    \"\"\"Estimate runtime and energy for computation\"\"\"\n",
    "    runtime = float(flops / HARDWARE_CONFIG['compute_throughput'])\n",
    "    energy = float(flops * HARDWARE_CONFIG['compute_efficiency'])\n",
    "    memory = float(act_size * HARDWARE_CONFIG['memory_energy'])\n",
    "    return {\n",
    "        'runtime': runtime,\n",
    "        'energy': energy + memory\n",
    "    }\n",
    "\n",
    "def estimate_communication_metrics(bytes_transferred: int) -> Dict[str, float]:\n",
    "    \"\"\"Estimate runtime and energy for data transfer\"\"\"\n",
    "    bits_transferred = bytes_transferred * 8\n",
    "    transfer_time = float(bits_transferred / HARDWARE_CONFIG['interconnect_bandwidth'])\n",
    "    total_latency = float(HARDWARE_CONFIG['interconnect_latency'] + transfer_time)\n",
    "    energy = float(bits_transferred * HARDWARE_CONFIG['interconnect_energy'])\n",
    "    return {\n",
    "        'runtime': total_latency,\n",
    "        'energy': energy\n",
    "    }\n",
    "\n",
    "def build_onnx_from_json(json_nodes, json_edges):\n",
    "    graph_nodes = []\n",
    "    graph_inputs = []\n",
    "    graph_outputs = []\n",
    "    initializers = []\n",
    "    node_map = {} \n",
    "\n",
    "    for node in json_nodes:\n",
    "        node_id = node['id']\n",
    "        op_type = node['opcode']\n",
    "        node_name = f\"node_{node_id}\"\n",
    "        node_map[node_id] = node_name\n",
    "        input_names = []\n",
    "        for edge in json_edges:\n",
    "            if edge['destination'] == node_id:\n",
    "                input_names.append(f\"node_{edge['source']}_output\")\n",
    "        output_name = f\"{node_name}_output\"\n",
    "\n",
    "        onnx_node = helper.make_node(\n",
    "            op_type=op_type,\n",
    "            inputs=input_names,\n",
    "            outputs=[output_name],\n",
    "            name=node_name\n",
    "        )\n",
    "        graph_nodes.append(onnx_node)\n",
    "\n",
    "        if 'param_shapes' in node and node['weight_shapes']:\n",
    "            for idx, shape in enumerate(node['weight_shapes']):\n",
    "                param_name = f\"{node_name}_param_{idx}\"\n",
    "                initializer = helper.make_tensor(\n",
    "                    name=param_name,\n",
    "                    data_type=onnx.TensorProto.FLOAT,\n",
    "                    dims=shape,\n",
    "                    vals=np.random.rand(*shape).astype(np.float32).flatten()\n",
    "                )\n",
    "                initializers.append(initializer)\n",
    "\n",
    "    for edge in json_edges:\n",
    "        if edge['source'] not in node_map:  \n",
    "            input_name = f\"node_{edge['source']}_output\"\n",
    "            graph_inputs.append(helper.make_tensor_value_info(\n",
    "                input_name,\n",
    "                onnx.TensorProto.FLOAT,\n",
    "                edge['tensor_shape']\n",
    "            ))\n",
    "        if edge['destination'] not in node_map:  \n",
    "            output_name = f\"node_{edge['destination']}_output\"\n",
    "            graph_outputs.append(helper.make_tensor_value_info(\n",
    "                output_name,\n",
    "                onnx.TensorProto.FLOAT,\n",
    "                edge['tensor_shape']\n",
    "            ))\n",
    "\n",
    "    graph = helper.make_graph(\n",
    "        nodes=graph_nodes,\n",
    "        name=\"ReconstructedGraph\",\n",
    "        inputs=graph_inputs,\n",
    "        outputs=graph_outputs,\n",
    "        initializer=initializers\n",
    "    )\n",
    "\n",
    "    model = helper.make_model(graph, producer_name=\"json_to_onnx\")\n",
    "    return model\n",
    "\n",
    "class EnhancedDAGExtractor:\n",
    "    def __init__(self):\n",
    "        self.nodes = []\n",
    "        self.edges = []\n",
    "        self.node_count = 0\n",
    "        self.tensor_shapes = {}\n",
    "        \n",
    "    \n",
    "    def get_node_id(self) -> int:\n",
    "        # self.node_count += 1\n",
    "        return self.node_count\n",
    "    \n",
    "    def add_node(self, name: str, op_type: str, weight_shape: Tuple[int, ...], \n",
    "                flops: int, input_shape: Tuple[int, ...], output_shape: Tuple[int, ...]) -> int:\n",
    "        node_id = self.get_node_id()\n",
    "        self.node_count+=1\n",
    "        weight_bytes = calculate_tensor_bytes(weight_shape, torch.int8) if weight_shape else 0\n",
    "        global weightsMem\n",
    "        global actMem\n",
    "        weightsMem += weight_bytes\n",
    "        if op_type in {\"Reshape3d\", \"Reshape4d\", \"Reshape5d\", \"Permute3d\", \"Permute4d\", \"Permute5d\", \"Transpose2d\",  \n",
    "                       \"Extract2dq\", \"Extract2dv\", \"Extract2dk\", \"ExtractInitial\", \"ExtractSecond\", \"ExtractThird\"}:\n",
    "            # Skip activation size for shape-changing operations without new allocations\n",
    "            activation_bytes = 0\n",
    "        else:\n",
    "            # Default: Include activation size\n",
    "            activation_bytes = calculate_tensor_bytes(output_shape, torch.int8) if output_shape else 0\n",
    "            actMem += activation_bytes\n",
    "\n",
    "        compute_metrics = estimate_compute_metrics(flops, activation_bytes)\n",
    "        \n",
    "        self.nodes.append({\n",
    "            \"id\": node_id,\n",
    "            \"name\": name,\n",
    "            \"opcode\": op_type,\n",
    "            \"weight_shape\": list(weight_shape) if weight_shape else [],\n",
    "            \"weight_size\": activation_bytes,\n",
    "            \"flops\": flops,\n",
    "            \"input_shape\": list(input_shape),\n",
    "            \"output_shape\": list(output_shape),\n",
    "            \"runtime\": compute_metrics['runtime'],\n",
    "            \"energy\": compute_metrics['energy']\n",
    "        })\n",
    "        return node_id\n",
    "    \n",
    "    def add_edge(self, source_id: int, dest_id: int, tensor_shape: Tuple[int, ...]):\n",
    "        tensor_bytes = calculate_tensor_bytes(tensor_shape)\n",
    "        comm_metrics = estimate_communication_metrics(tensor_bytes)\n",
    "        \n",
    "        self.edges.append({\n",
    "            \"source\": source_id,\n",
    "            \"destination\": dest_id,\n",
    "            \"tensor_shape\": list(tensor_shape),\n",
    "            \"tensor_size\": tensor_bytes,\n",
    "            \"latency\": comm_metrics['runtime'],\n",
    "            \"energy\": comm_metrics['energy']\n",
    "        })\n",
    "\n",
    "    def _extract_first_tensor_shape(self, data):\n",
    "        if isinstance(data, torch.Tensor):\n",
    "            return tuple(data.shape)\n",
    "        \n",
    "        if isinstance(data, (tuple, list)) and len(data) > 0:\n",
    "            return self._extract_first_tensor_shape(data[0])\n",
    "        \n",
    "        return None\n",
    "\n",
    "    def _add_edges_for_nested_input(self, inp, dest_id):\n",
    "        if isinstance(inp, torch.Tensor):\n",
    "            if inp in self.tensor_shapes:\n",
    "                source_id, tensor_shape = self.tensor_shapes[inp]\n",
    "                self.add_edge(source_id, dest_id, tensor_shape)\n",
    "            else:\n",
    "                pass  # Ignore tensors not seen before\n",
    "        elif isinstance(inp, (tuple, list)):\n",
    "            for i in inp:\n",
    "                self._add_edges_for_nested_input(i, dest_id)\n",
    "\n",
    "    def hook_fn(self, module, input_tensor, output_tensor):\n",
    "        node_id = self.get_node_id()\n",
    "        op_type = module.__class__.__name__\n",
    "        \n",
    "        # input_shape = tuple(input_tensor[0].shape)\n",
    "        # output_shape = tuple(output_tensor.shape)\n",
    "        # weight_shape = tuple(module.weight.shape) if hasattr(module, 'weight') else None\n",
    "        \n",
    "        input_shape = self._extract_first_tensor_shape(input_tensor)\n",
    "        output_shape = self._extract_first_tensor_shape(output_tensor)\n",
    "        weight_shape = tuple(module.weight.shape) if hasattr(module, 'weight') else None\n",
    "\n",
    "        flops = count_flops(module, input_shape, output_shape)\n",
    "\n",
    "        \n",
    "        self.add_node(\n",
    "            name=f\"{op_type}_{node_id}\",\n",
    "            op_type=op_type,\n",
    "            weight_shape=weight_shape,\n",
    "            flops=flops,\n",
    "            input_shape=input_shape,\n",
    "            output_shape=output_shape\n",
    "        )\n",
    "        \n",
    "        self.tensor_shapes[output_tensor] = (node_id, output_shape)\n",
    "\n",
    "        for inp in input_tensor:\n",
    "            # if inp in self.tensor_shapes:\n",
    "            #     source_id, tensor_shape = self.tensor_shapes[inp]\n",
    "            #     self.add_edge(source_id, node_id, tensor_shape)\n",
    "            self._add_edges_for_nested_input(inp, node_id)\n",
    "\n",
    "    def is_shape_tuple(self, x):\n",
    "        \"\"\"\n",
    "        Returns True if x is a tuple/list of ints, e.g. (1, 3, 224, 224).\n",
    "        Returns False otherwise.\n",
    "        \"\"\"\n",
    "        if not isinstance(x, (tuple, list)):\n",
    "            return False\n",
    "        return all(isinstance(el, int) for el in x)\n",
    "\n",
    "\n",
    "    def extract_dag(self, model: nn.Module, input_size: Union[Tuple[int, ...], List[torch.Tensor]]):\n",
    "        hooks = []\n",
    "        for name, module in model.named_modules():\n",
    "            if isinstance(module, (nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.ReLU, \n",
    "                                nn.MaxPool2d, nn.AvgPool2d, nn.ReLU6, ResidualAdd, nn.GELU, nn.AvgPool2d, \n",
    "                                depth_encoder.LayerNorm, depth_encoder.MatrixMultiply, depth_encoder.Softmax, depth_encoder.WeightedSum,\n",
    "                                depth_encoder.GammaMultiply, DropPath, depth_encoder.PosEncode,\n",
    "                                depth_encoder.Reshape3d, depth_encoder.Reshape4d, depth_encoder.Reshape5d,\n",
    "                                depth_encoder.Permute3d, depth_encoder.Permute4d, depth_encoder.Permute5d, \n",
    "                                depth_encoder.Transpose2d, depth_encoder.Normalize2d, depth_encoder.Cat,\n",
    "                                depth_encoder.Extract2dq, depth_encoder.Extract2dv, depth_encoder.Extract2dk,\n",
    "                                nn.ReflectionPad2d, nn.ZeroPad2d, nn.ELU, nn.Sigmoid, Upsampling, ExtractInitial,\n",
    "                                ExtractSecond, ExtractThird)):\n",
    "                # if(isinstance(module, ResidualAdd)):\n",
    "                #     print(\"This is in extract DAG for ResidualAdd\")\n",
    "                hooks.append(module.register_forward_hook(self.hook_fn))\n",
    "        \n",
    "        if self.is_shape_tuple(input_size):\n",
    "            print(f\"Creating dummy input tensor of size: {input_size}\")\n",
    "            dummy_input = torch.randn(input_size)\n",
    "        else:\n",
    "            print(\"Using provided input tensor(s)\")\n",
    "            dummy_input = input_size\n",
    "\n",
    "        # # Handle both single tensor and list of tensor inputs\n",
    "        # if isinstance(input_size, (tuple, list)) and isinstance(input_size[0], torch.Tensor):\n",
    "        #     dummy_input = input_size  # Use provided tensors directly\n",
    "        # else:\n",
    "        #     dummy_input = torch.randn(input_size)  # Create new tensor\n",
    "            \n",
    "        model(dummy_input)\n",
    "        # print(\"Ran model with dummy input to extract DAG\")\n",
    "\n",
    "        \n",
    "        for hook in hooks:\n",
    "            hook.remove()\n",
    "\n",
    "        # print(\"Removed hooks after extracting DAG\")\n",
    "\n",
    "        onnx_model = build_onnx_from_json(self.nodes, self.edges)\n",
    "\n",
    "        # print(\"built onnx model\")\n",
    "\n",
    "        onnx.save(onnx_model, \"reconstructed_model_depthencoder.onnx\")\n",
    "        \n",
    "        # print(f\"Num Nodes: {len(self.nodes)}, Num Edges: {len(self.edges)}\")\n",
    "        return {\n",
    "            \"nodes\": self.nodes,\n",
    "            \"edges\": self.edges,\n",
    "            \"hardware_config\": HARDWARE_CONFIG\n",
    "        }\n",
    "\n",
    "def analyze_model(model_name: str, model: nn.Module, input_size: Tuple[int, ...]):\n",
    "    extractor = EnhancedDAGExtractor()\n",
    "    dag = extractor.extract_dag(model, input_size)\n",
    "\n",
    "    print(f\"Extracted DAG for {model_name}\")\n",
    "    print(f\"Num Nodes: {len(dag['nodes'])}, Num Edges: {len(dag['edges'])}\")\n",
    "    \n",
    "    with open(f'{model_name}_dag_enhanced.json', 'w') as f:\n",
    "        json.dump(dag, f, indent=2, cls=NumpyFloatEncoder)\n",
    "\n",
    "    print(f\"Total Flops using Systolic Array: {systolicFlops}\")\n",
    "    print(f\"Total Flops using Spatz Vector Unit: {spatzFlops}\")\n",
    "    print(f\"Total Weights Memory: {weightsMem}\")\n",
    "    print(f\"Total Activation Memory: {actMem}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# input_features = torch.randn(1, 6, 224, 224)\n",
    "\n",
    "# # # Analyze ResNet18\n",
    "# resnet18 = ResnetEncoder(num_layers=18, pretrained=False, num_input_images=2)\n",
    "\n",
    "# encoder_features = resnet18(input_features)\n",
    "\n",
    "# # # # resnet18 = models.resnet18(pretrained=False)\n",
    "# # analyze_model('resnet18', resnet18, (1, 6, 224, 224))\n",
    "\n",
    "# # # Analyze PoseDecoder\n",
    "# # # num_ch_enc = np.array([64, 64, 128, 256, 512])  # Example encoder channels\n",
    "# num_input_features = 1  # Add this parameter\n",
    "# pose_decoder = PoseDecoder(\n",
    "#     num_ch_enc=resnet18.num_ch_enc,\n",
    "#     num_input_features=num_input_features, \n",
    "#     num_frames_to_predict_for=2\n",
    "# )\n",
    "# # # Create dummy input features list\n",
    "# pose_input_features = [\n",
    "#     encoder_features\n",
    "# ]\n",
    "\n",
    "# # print(\"Pose input tensor shape: \", len(encoder_features))\n",
    "\n",
    "# analyze_model('pose_decoder', pose_decoder, pose_input_features)\n",
    "\n",
    "# Analyze DepthDecoder\n",
    "# depth_decoder = DepthDecoder(\n",
    "#     num_ch_enc=num_ch_enc,\n",
    "#     scales=range(4),\n",
    "#     num_output_channels=1,\n",
    "#     use_skips=True\n",
    "# )\n",
    "# # Create dummy input features list\n",
    "# depth_input_features = [\n",
    "#     torch.randn(1, 64, 56, 56),    # First encoder feature\n",
    "#     torch.randn(1, 64, 28, 28),    # Second encoder feature\n",
    "#     torch.randn(1, 128, 14, 14),   # Third encoder feature\n",
    "#     torch.randn(1, 256, 7, 7),     # Fourth encoder feature\n",
    "#     torch.randn(1, 512, 7, 7)      # Fifth encoder feature\n",
    "# ]\n",
    "# analyze_model('depth_decoder', depth_decoder, depth_input_features)\n",
    "\n",
    "input_features = torch.randn(1, 3, 224, 224)\n",
    "\n",
    "process_batch = ProcessBatch()\n",
    "\n",
    "# input_shape = (3, 224, 224)\n",
    "# with torch.no_grad():\n",
    "#     macs, params = get_model_complexity_info(\n",
    "#         process_batch,\n",
    "#         input_shape,\n",
    "#         as_strings=True,\n",
    "#         print_per_layer_stat=True\n",
    "#     )\n",
    "\n",
    "# print(f\"Computational complexity: {macs}\")  \n",
    "\n",
    "# with profile(\n",
    "#     activities=[ProfilerActivity.CPU],\n",
    "#     on_trace_ready=tensorboard_trace_handler(\"./log_dir\"),\n",
    "#     record_shapes=True,\n",
    "#     with_stack=True,\n",
    "#     with_flops=True\n",
    "# ) as prof:\n",
    "#     process_batch(input_features)\n",
    "#analyze_model('process_batch', process_batch, (1, 3, 224, 224))\n",
    "\n",
    "print(summary(process_batch, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\"],depth=5))\n",
    "\n",
    "# summary(process_batch, input_size=(1, 3, 224, 224), col_names=[\"input_size\", \"output_size\", \"num_params\"],depth=5)\n",
    "\n",
    "# print(\"Get Model Complexity Info\")\n",
    "# with torch.no_grad():\n",
    "#     macs, params = get_model_complexity_info(\n",
    "#         process_batch,\n",
    "#         (3, 224, 224),\n",
    "#         as_strings=True,\n",
    "#         print_per_layer_stat=True,\n",
    "#         verbose=True\n",
    "#     )\n",
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "\n",
    "# flop_count = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "# print(f\"Total FLOPs: {flop_count}\")\n",
    "\n",
    "# process_pose = ProcessPose()\n",
    "# input_features = torch.randn(1, 6, 224, 224)\n",
    "# with profile(\n",
    "#     activities=[ProfilerActivity.CPU],\n",
    "#     on_trace_ready=tensorboard_trace_handler(\"./log_dir\"),\n",
    "#     record_shapes=True,\n",
    "#     with_stack=True,\n",
    "#     with_flops=True\n",
    "# ) as prof:\n",
    "#     process_pose(input_features)\n",
    "# analyze_model('process_batch', process_batch, (1, 3, 224, 224))\n",
    "systolicFlops = 0\n",
    "spatzFlops = 0  \n",
    "weightsMem = 0\n",
    "actMem = 0\n",
    "\n",
    "\n",
    "# print(prof.key_averages().table(sort_by=\"cpu_time_total\", row_limit=10))\n",
    "#process_pose = ProcessPose()\n",
    "# flop_count = sum([event.flops for event in prof.key_averages() if event.flops is not None])\n",
    "# print(f\"Total FLOPs: {flop_count}\")\n",
    "\n",
    "# analyze_model('process_pose', process_pose, (1, 6, 224, 224))\n",
    "\n",
    "# depthencoder = depth_encoder.LiteMono() \n",
    "\n",
    "# # analyze_model('depth_encoder', depthencoder, (1, 3, 224, 224))\n",
    "\n",
    "# output_encoder = depthencoder(input_features)\n",
    "\n",
    "# depth_decoder = DepthDecoder(depthencoder.num_ch_enc, scales = range(3))\n",
    "\n",
    "# analyze_model('depth_decoder', depth_decoder, output_encoder)\n",
    "\n",
    "# print(depth_decoder(output_encoder))\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
