{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision_updated'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mresnet_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ResnetEncoder\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdepth_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DepthDecoder\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpose_decoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PoseDecoder\n",
      "File \u001b[0;32m/sim/sadayo24/Lite-Mono/networks/resnet_encoder.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision_updated\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_zoo\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmodel_zoo\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision_updated\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision_updated'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from resnet_encoder import ResnetEncoder\n",
    "from depth_decoder import DepthDecoder\n",
    "from pose_decoder import PoseDecoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resource Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENERGY_PER_FLOP = 1e-12\n",
    "TIME_PER_FLOP = 1e-9\n",
    "COMM_BANDWIDTH = 1e10\n",
    "ENERGY_PER_BYTE = 1e-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_flops(module, input, output):\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        kernel_ops = np.prod(module.kernel_size)\n",
    "        cin = module.in_channels \n",
    "        cout = module.out_channels\n",
    "        hout, wout = output.shape[2], output.shape[3]\n",
    "        flops = 2 * kernel_ops * cin * cout * hout * wout\n",
    "        return flops\n",
    "    elif isinstance(module, nn.Linear):\n",
    "        flops = 2 * module.in_features * module.out_features\n",
    "        return flops\n",
    "    elif isinstance(module, (nn.ReLU, nn.ELU, nn.GELU)):\n",
    "        if isinstance(module, nn.ReLU):\n",
    "            flops = output.numel()\n",
    "        elif isinstance(module, nn.ELU):\n",
    "            flops = 4*output.numel()\n",
    "        else: \n",
    "            flops = 12*output.numel()\n",
    "        return flops\n",
    "    elif isinstance(module, nn.BatchNorm2d):\n",
    "        flops = 4*output.numel()\n",
    "        return flops \n",
    "    elif isinstance(module, (nn.MaxPool2d, nn.AvgPool2d)):\n",
    "        if isinstance(module, nn.MaxPool2d):\n",
    "            kernel_ops = np.prod(module.kernel_size) - 1\n",
    "        else:\n",
    "            kernel_ops = np.prod(module.kernel_size)\n",
    "        flops = kernel_ops*output.numel()\n",
    "        return flops\n",
    "    return 0\n",
    "\n",
    "def hook_fn(module, input, output):\n",
    "    global node_id\n",
    "    flops = compute_flops(module, input, output)\n",
    "    param_shapes = [list(p.shape) for p in module.parameters() if hasattr(module, 'parameters')]\n",
    "    \n",
    "    node = {\n",
    "        \"name\": module.__class__.__name__,\n",
    "        \"id\": node_id,\n",
    "        \"opcode\": type(module).__name__,\n",
    "        \"param_shapes\": param_shapes,  \n",
    "        \"energy\": flops * ENERGY_PER_FLOP,\n",
    "        \"runtime\": flops * TIME_PER_FLOP,\n",
    "        \"flops\": flops,\n",
    "        \"size\": sum(p.numel() * p.element_size() for p in module.parameters() if hasattr(module, 'parameters')),\n",
    "    }\n",
    "    nodes.append(node)\n",
    "\n",
    "    if isinstance(output, torch.Tensor):\n",
    "        tensor_to_node[output] = node_id\n",
    "    elif isinstance(output, (tuple, list)):\n",
    "        for out in output:\n",
    "            if isinstance(out, torch.Tensor):\n",
    "                tensor_to_node[out] = node_id\n",
    "\n",
    "    if isinstance(input, (tuple, list)):\n",
    "        for inp in input:\n",
    "            if isinstance(inp, torch.Tensor) and inp in tensor_to_node:\n",
    "                source_id = tensor_to_node[inp]\n",
    "                if source_id != node_id:\n",
    "                    data_volume = inp.numel() * inp.element_size()\n",
    "                    edge = {\n",
    "                        \"source\": source_id,\n",
    "                        \"destination\": node_id,\n",
    "                        \"shape\": list(inp.shape),\n",
    "                        \"latency\": data_volume / COMM_BANDWIDTH,\n",
    "                        \"energy\": data_volume * ENERGY_PER_BYTE,\n",
    "                        \"size\": data_volume\n",
    "                    }\n",
    "                    edges.append(edge)\n",
    "    node_id += 1\n",
    "\n",
    "def convert_to_serializable(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {key: convert_to_serializable(value) for key, value in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_to_serializable(item) for item in obj]\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.generic):\n",
    "        return obj.item()\n",
    "    elif isinstance(obj, torch.Tensor):\n",
    "        return obj.tolist()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# With Learnable Parameters\n",
    "m = nn.BatchNorm2d(100)\n",
    "rel = nn.ReLU(inplace=False)\n",
    "\n",
    "# Without Learnable Parameters\n",
    "# m = nn.BatchNorm2d(100, affine=False)\n",
    "input = torch.randn(20, 100, 35, 45)\n",
    "output = m(input)\n",
    "output = rel(output)\n",
    "print(isinstance(output, torch.Tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ONNX Reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "import onnx.helper as helper\n",
    "import numpy as np\n",
    "\n",
    "def build_onnx_from_json(json_nodes, json_edges):\n",
    "    graph_nodes = []\n",
    "    graph_inputs = []\n",
    "    graph_outputs = []\n",
    "    initializers = []\n",
    "    node_map = {} \n",
    "\n",
    "    for node in json_nodes:\n",
    "        node_id = node['id']\n",
    "        op_type = node['opcode']\n",
    "        node_name = f\"node_{node_id}\"\n",
    "        node_map[node_id] = node_name\n",
    "        input_names = []\n",
    "        for edge in json_edges:\n",
    "            if edge['destination'] == node_id:\n",
    "                input_names.append(f\"node_{edge['source']}_output\")\n",
    "        output_name = f\"{node_name}_output\"\n",
    "\n",
    "        onnx_node = helper.make_node(\n",
    "            op_type=op_type,\n",
    "            inputs=input_names,\n",
    "            outputs=[output_name],\n",
    "            name=node_name\n",
    "        )\n",
    "        graph_nodes.append(onnx_node)\n",
    "\n",
    "        if 'param_shapes' in node and node['param_shapes']:\n",
    "            for idx, shape in enumerate(node['param_shapes']):\n",
    "                param_name = f\"{node_name}_param_{idx}\"\n",
    "                initializer = helper.make_tensor(\n",
    "                    name=param_name,\n",
    "                    data_type=onnx.TensorProto.FLOAT,\n",
    "                    dims=shape,\n",
    "                    vals=np.random.rand(*shape).astype(np.float32).flatten()\n",
    "                )\n",
    "                initializers.append(initializer)\n",
    "\n",
    "    for edge in json_edges:\n",
    "        if edge['source'] not in node_map:  \n",
    "            input_name = f\"node_{edge['source']}_output\"\n",
    "            graph_inputs.append(helper.make_tensor_value_info(\n",
    "                input_name,\n",
    "                onnx.TensorProto.FLOAT,\n",
    "                edge['shape']\n",
    "            ))\n",
    "        if edge['destination'] not in node_map:  \n",
    "            output_name = f\"node_{edge['destination']}_output\"\n",
    "            graph_outputs.append(helper.make_tensor_value_info(\n",
    "                output_name,\n",
    "                onnx.TensorProto.FLOAT,\n",
    "                edge['shape']\n",
    "            ))\n",
    "\n",
    "    graph = helper.make_graph(\n",
    "        nodes=graph_nodes,\n",
    "        name=\"ReconstructedGraph\",\n",
    "        inputs=graph_inputs,\n",
    "        outputs=graph_outputs,\n",
    "        initializer=initializers\n",
    "    )\n",
    "\n",
    "    model = helper.make_model(graph, producer_name=\"json_to_onnx\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resnet Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "edges = []\n",
    "node_id = 0\n",
    "tensor_to_node = {}\n",
    "\n",
    "num_layers = 18  \n",
    "pretrained = False  \n",
    "num_input_images = 2\n",
    "\n",
    "encoder = ResnetEncoder(num_layers=num_layers, pretrained=pretrained, num_input_images=num_input_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in encoder.encoder.modules():\n",
    "    if len(list(module.children())) == 0:\n",
    "        module.register_forward_hook(hook_fn)\n",
    "\n",
    "input_image = torch.randn(1, 6, 224, 224)\n",
    "encoder(input_image)\n",
    "\n",
    "data = {\"nodes\": [convert_to_serializable(node) for node in nodes], \"edges\": [convert_to_serializable(edge) for edge in edges]}\n",
    "with open('resnet_encoder_graph.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "onnx_model = build_onnx_from_json(nodes, edges)\n",
    "onnx.save(onnx_model, \"reconstructed_model.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DepthDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = []\n",
    "edges = []\n",
    "tensor_to_node = {}\n",
    "node_id = 0\n",
    "\n",
    "num_ch_enc = np.array([48, 80, 128])  # Adjusted to match LiteMono encoder\n",
    "\n",
    "    # Instantiate the DepthDecoder with adjusted scales\n",
    "depth_decoder= DepthDecoder(num_ch_enc=num_ch_enc, scales=range(len(num_ch_enc)))\n",
    "# depth_decoder = DepthDecoder(num_ch_enc=num_ch_enc, scales=scales, num_output_channels=1, use_skips=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for module in depth_decoder.modules():\n",
    "    if len(list(module.children())) == 0:  \n",
    "        module.register_forward_hook(hook_fn)\n",
    "\n",
    "batch_size = 1\n",
    "height = 224\n",
    "width = 224\n",
    "\n",
    "\n",
    "factors = [4, 8, 16]\n",
    "\n",
    "input_features = [\n",
    "        torch.randn(batch_size, num_ch_enc[i], height // factors[i], width // factors[i])\n",
    "        for i in range(len(num_ch_enc))\n",
    "    ]\n",
    "\n",
    "\n",
    "depth_decoder(input_features)\n",
    "\n",
    "data = {\"nodes\": [convert_to_serializable(node) for node in nodes], \"edges\": [convert_to_serializable(edge) for edge in edges]}\n",
    "with open('depth_decoder_graph.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "onnx_model = build_onnx_from_json(nodes, edges)\n",
    "onnx.save(onnx_model, \"reconstructed_depth_decoder_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_decoder = PoseDecoder(num_ch_enc=encoder.num_ch_enc, num_input_features=1, num_frames_to_predict_for=2)\n",
    "input_features = [encoder(input_image)]\n",
    "\n",
    "for module in pose_decoder.modules():\n",
    "    if len(list(module.children())) == 0:  \n",
    "        module.register_forward_hook(hook_fn)\n",
    "\n",
    "data = {\"nodes\": [convert_to_serializable(node) for node in nodes], \"edges\": [convert_to_serializable(edge) for edge in edges]}\n",
    "with open('pose_decoder_graph.json', 'w') as f:\n",
    "    json.dump(data, f, indent=4)\n",
    "\n",
    "onnx_model = build_onnx_from_json(nodes, edges)\n",
    "onnx.save(onnx_model, \"reconstructed_pose_model.onnx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
